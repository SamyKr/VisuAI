//
//  VoiceInteractionManager.swift
//  VizAI Vision
//
//  üé§ GESTIONNAIRE D'INTERACTION VOCALE INTELLIGENTE
//
//  R√îLE DANS L'ARCHITECTURE :
//  - Point d'entr√©e unique pour toutes les interactions vocales utilisateur
//  - Interface entre la reconnaissance vocale Apple et le syst√®me de r√©ponse contextuelle
//  - Analyseur s√©mantique des questions en fran√ßais naturel
//  - G√©n√©rateur de r√©ponses adapt√©es au contexte visuel d√©tect√©
//
//  FONCTIONNALIT√âS PRINCIPALES :
//  ‚úÖ Reconnaissance vocale 100% locale (aucune donn√©e envoy√©e sur internet)
//  ‚úÖ Questions sp√©cialis√©es aide √† la travers√©e ("Puis-je traverser ?")
//  ‚úÖ Analyse de sc√®ne par plans de profondeur (proche/moyen/loin)
//  ‚úÖ Dictionnaire complet fran√ßais-anglais (49 types d'objets)
//  ‚úÖ Parsing intelligent avec synonymes et variantes linguistiques
//  ‚úÖ Gestion d'erreurs robuste avec r√©cup√©ration automatique
//  ‚úÖ Support AirPods et routage audio intelligent
//
//  TYPES DE QUESTIONS SUPPORT√âES :
//  1. Pr√©sence : "Y a-t-il une voiture ?"
//  2. Comptage : "Combien de personnes ?"
//  3. Localisation : "O√π est le feu ?"
//  4. Description : "D√©cris la sc√®ne"
//  5. Travers√©e : "Puis-je traverser ?" (analyse s√©curit√©)
//  6. Vue d'ensemble : "Qu'est-ce qui m'entoure ?"

import Foundation
import AVFoundation
import Speech
import UIKit

// MARK: - Structures de Donn√©es

enum QuestionType {
    case presence, count, location, description, sceneOverview, specific, crossing, unknown
}

struct ParsedQuestion {
    let type: QuestionType
    let targetObject: String?
    let confidence: Float
    let originalText: String
}

struct SceneAnalysis {
    let totalObjects: Int
    let objectsByType: [String: Int]
    let objectsByZone: [String: [String]]
    let distances: [String: Float]
    let criticalObjects: [String]
    let navigationObjects: [String]
}

// MARK: - Gestionnaire Principal

class VoiceInteractionManager: NSObject, ObservableObject {
    
    // MARK: - Configuration
    private let emergencyTimeoutDuration: TimeInterval = 30.0
    private let maxRetryAttempts = 3
    private let retryDelay: TimeInterval = 2.0
    
    // MARK: - √âtat Observable
    @Published var isListening = false
    @Published var isWaitingForQuestion = false
    @Published var lastRecognizedText = ""
    @Published var interactionEnabled = true
    
    // MARK: - Gestion d'Erreurs
    private var currentRetryCount = 0
    private var lastErrorTime: Date?
    private var isRecovering = false
    private var speechAvailable = true
    
    // MARK: - Reconnaissance Vocale Apple
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "fr-FR"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    // MARK: - Audio et Timers
    private var beepPlayer: AVAudioPlayer?
    private var questionTimer: Timer?
    private var recoveryTimer: Timer?
    
    // MARK: - Gestion R√©sultats Partiels
    private var lastPartialUpdate: Date?
    private var lastPartialResult: String = ""
    private var partialResultTimer: Timer?
    
    // MARK: - Syst√®me de M√©morisation 3 Secondes
    private var isCollectingObjects = false
    private var collectedObjects: [(object: TrackedObject, score: Float, timestamp: Date)] = []
    private var collectionTimer: Timer?
    private var pendingQuestion: ParsedQuestion?
    private var collectionStartTime: Date?
    
    // MARK: - R√©f√©rences Externes
    weak var voiceSynthesisManager: VoiceSynthesisManager?
    private var currentImportantObjects: [(object: TrackedObject, score: Float)] = []
    
    // MARK: - Dictionnaire de Traduction Complet (49 objets)
    private let objectTranslations: [String: [String]] = [
        // V√©hicules et usagers
        "person": ["personne", "personnes", "gens", "pi√©ton", "pi√©tons", "homme", "femme", "enfant", "individu"],
        "cyclist": ["cycliste", "cyclistes", "v√©lo", "cyclisme"],
        "motorcyclist": ["motocycliste", "motocyclistes", "motard", "motards"],
        "car": ["voiture", "voitures", "auto", "autos", "v√©hicule", "v√©hicules", "bagnole", "caisse", "automobile"],
        "truck": ["camion", "camions", "poids lourd", "poids lourds", "semi", "semi-remorque", "camionnette"],
        "bus": ["bus", "autobus", "car", "transport en commun"],
        "motorcycle": ["moto", "motos", "motocyclette", "motocyclettes", "scooter", "scooters", "deux-roues"],
        "bicycle": ["v√©lo", "v√©los", "bicyclette", "bicyclettes", "bike", "cycle"],
        "slow_vehicle": ["v√©hicule lent", "v√©hicule ralenti", "voiture lente"],
        "vehicle_group": ["groupe de v√©hicules", "convoi", "files de voitures"],
        "rail_vehicle": ["v√©hicule ferroviaire", "train", "tramway", "m√©tro"],
        "boat": ["bateau", "bateaux", "embarcation", "navire"],
        
        // Infrastructure routi√®re
        "sidewalk": ["trottoir", "trottoirs", "chauss√©e pi√©tonne"],
        "road": ["route", "routes", "rue", "rues", "chauss√©e", "voie", "avenue", "boulevard"],
        "crosswalk": ["passage pi√©ton", "passage pi√©tons", "passage clout√©", "zebra", "travers√©e"],
        "driveway": ["all√©e", "all√©es", "entr√©e", "acc√®s"],
        "bike_lane": ["piste cyclable", "voie cyclable", "bande cyclable"],
        "parking_area": ["zone de stationnement", "parking", "place de parking", "stationnement"],
        "rail_track": ["voie ferr√©e", "rails", "chemin de fer"],
        "service_lane": ["voie de service", "bande de service"],
        "curb": ["bordure", "bordures", "trottoir", "rebord"],
        
        // Barri√®res et obstacles
        "wall": ["mur", "murs", "muraille", "cloison"],
        "fence": ["cl√¥ture", "cl√¥tures", "grillage", "barri√®re"],
        "guard_rail": ["glissi√®re de s√©curit√©", "garde-corps", "barri√®re de s√©curit√©"],
        "temporary_barrier": ["barri√®re temporaire", "barri√®re de chantier", "obstacle temporaire"],
        "barrier_other": ["autre barri√®re", "obstacle", "barri√®re"],
        "barrier": ["barri√®re", "barri√®res", "obstacle", "obstacles"],
        "pole": ["poteau", "poteaux", "pilier", "piliers", "m√¢t", "borne"],
        
        // Signalisation et √©quipements
        "traffic_light": ["feu", "feux", "feu de circulation", "feux de circulation", "feu tricolore", "signal", "signalisation lumineuse"],
        "traffic_sign": ["panneau", "panneaux", "panneau de signalisation", "signalisation", "stop", "signal routier"],
        "street_light": ["lampadaire", "lampadaires", "√©clairage public", "r√©verb√®re"],
        "traffic_cone": ["c√¥ne", "c√¥nes", "plot", "balise"],
        
        // Mobilier urbain
        "bench": ["banc", "bancs", "si√®ge"],
        "trash_can": ["poubelle", "poubelles", "benne", "conteneur"],
        "fire_hydrant": ["bouche d'incendie", "borne incendie", "hydrant"],
        "mailbox": ["bo√Æte aux lettres", "bo√Æte postale", "courrier"],
        "parking_meter": ["parcm√®tre", "horodateur", "compteur parking"],
        "bike_rack": ["support √† v√©los", "rack v√©lo", "stationnement v√©lo"],
        "phone_booth": ["cabine t√©l√©phonique", "cabine", "t√©l√©phone public"],
        
        // √âl√©ments de voirie
        "pothole": ["nid-de-poule", "trou", "d√©faut chauss√©e"],
        "manhole": ["plaque d'√©gout", "bouche d'√©gout", "regard"],
        "catch_basin": ["regard d'√©gout", "avaloir", "grille d'√©vacuation"],
        "water_valve": ["vanne d'eau", "robinet", "valve"],
        "junction_box": ["bo√Ætier de jonction", "coffret √©lectrique", "bo√Ætier"],
        
        // Structures et environnement
        "building": ["b√¢timent", "b√¢timents", "immeuble", "immeubles", "maison", "maisons", "construction"],
        "bridge": ["pont", "ponts", "passerelle", "viaduc"],
        "tunnel": ["tunnel", "tunnels", "passage souterrain"],
        "garage": ["garage", "garages", "abri"],
        "vegetation": ["v√©g√©tation", "plante", "plantes", "verdure", "feuillage"],
        "water": ["eau", "rivi√®re", "lac", "√©tang", "cours d'eau"],
        "terrain": ["terrain", "sol", "surface", "ground"],
        "animals": ["animaux", "animal", "b√™te", "b√™tes"]
    ]
    
    // MARK: - Mots-cl√©s Questions
    private func getPresenceKeywords() -> [String] {
        return ["y a-t-il", "ya-t-il", "est-ce qu'il y a", "il y a", "vois-tu", "d√©tectes-tu","tu vois"]
    }
    
    private func getCountKeywords() -> [String] {
        return ["combien", "nombre", "quantit√©"]
    }
    
    private func getLocationKeywords() -> [String] {
        return ["o√π", "position", "situ√©e", "situ√©", "place", "localisation", "emplacement"]
    }
    
    private func getDescriptionKeywords() -> [String] {
        return ["qu'est-ce qui", "que vois-tu", "devant moi", "autour"]
    }
    
    private func getCrossingKeywords() -> [String] {
        return ["peux traverser", "puis traverser", "peut traverser", "traverser", "passer", "croiser", "s√ªr de traverser", "s√©curis√© pour traverser"]
    }
    
    private func getSceneKeywords() -> [String] {
        return ["d√©cris", "d√©cris la sc√®ne", "que se passe-t-il", "situation"]
    }
    
    private func getQuestionKeywords() -> [QuestionType: [String]] {
        return [
            .presence: getPresenceKeywords(),
            .count: getCountKeywords(),
            .location: getLocationKeywords(),
            .description: getDescriptionKeywords(),
            .sceneOverview: getSceneKeywords(),
            .crossing: getCrossingKeywords()
        ]
    }
    
    // MARK: - Initialisation
    
    override init() {
        super.init()
        setupAudio()
        requestSpeechPermission()
        setupBeepSound()
        checkSpeechAvailability()
    }
    
    deinit {
        cleanupResources()
    }
    
    // MARK: - Configuration Audio Intelligente
    
    /// Configure la session audio avec support AirPods et Bluetooth
    private func setupAudio() {
        do {
            let audioSession = AVAudioSession.sharedInstance()
            
            try audioSession.setCategory(
                .playAndRecord,
                mode: .measurement,
                options: [
                    .duckOthers,
                    .allowBluetooth,
                    .allowBluetoothA2DP,
                ]
            )
            
            try audioSession.setActive(false)
            
            // Route audio intelligente selon les p√©riph√©riques connect√©s
            let hasAirPods = audioSession.currentRoute.outputs.contains {
                $0.portType == .bluetoothA2DP || $0.portType == .bluetoothHFP
            }
            
            if hasAirPods {
                try audioSession.overrideOutputAudioPort(.none) // AirPods
            } else {
                try audioSession.overrideOutputAudioPort(.speaker) // Haut-parleurs
            }
            
        } catch {
            print("‚ùå Erreur configuration audio: \(error)")
        }
    }
    
    /// V√©rifie la disponibilit√© de la reconnaissance locale s√©curis√©e
    private func checkSpeechAvailability() {
        guard let speechRecognizer = speechRecognizer else {
            speechAvailable = false
            interactionEnabled = false
            return
        }
        
        speechAvailable = speechRecognizer.isAvailable
        
        if !speechAvailable {
            interactionEnabled = false
            return
        }
        
        // V√©rification reconnaissance locale obligatoire (s√©curit√©/confidentialit√©)
        if #available(iOS 13.0, *) {
            if speechRecognizer.supportsOnDeviceRecognition {
                interactionEnabled = true
            } else {
                interactionEnabled = false
            }
        } else {
            interactionEnabled = false
        }
    }
    
    /// Nettoie toutes les ressources audio et timers
    private func cleanupResources() {
        stopListening()
        resetCollection() // Nettoyer le syst√®me de collection
        recoveryTimer?.invalidate()
        recoveryTimer = nil
        
        do {
            try AVAudioSession.sharedInstance().setActive(false)
        } catch {
            // Ignorer les erreurs de nettoyage
        }
    }
    
    // MARK: - Son d'Activation Personnalis√©
    
    /// Configure le son d'activation (personnalis√© ou g√©n√©r√©)
    private func setupBeepSound() {
        let customSoundURL = getCustomSoundURL()
        
        do {
            if FileManager.default.fileExists(atPath: customSoundURL.path) {
                beepPlayer = try AVAudioPlayer(contentsOf: customSoundURL)
            } else {
                let beepURL = createBeepSound()
                beepPlayer = try AVAudioPlayer(contentsOf: beepURL)
            }
            
            beepPlayer?.prepareToPlay()
            beepPlayer?.volume = 1.0
        } catch {
            print("‚ùå Erreur cr√©ation son: \(error)")
        }
    }
    
    /// Recherche un fichier son personnalis√© dans le bundle ou Documents
    /// @return URL du fichier son √† utiliser
    private func getCustomSoundURL() -> URL {
        let possibleBaseNames = ["indicvoca"]
        let possibleExtensions = ["wav", "mp3", "m4a"]
        
        // Recherche dans le bundle
        for baseName in possibleBaseNames {
            for ext in possibleExtensions {
                if let bundleURL = Bundle.main.url(forResource: baseName, withExtension: ext) {
                    return bundleURL
                }
            }
        }
        
        // Recherche dans Documents
        let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        for baseName in possibleBaseNames {
            for ext in possibleExtensions {
                let soundURL = documentsPath.appendingPathComponent("\(baseName).\(ext)")
                if FileManager.default.fileExists(atPath: soundURL.path) {
                    return soundURL
                }
            }
        }
        
        // Fallback vers bip g√©n√©r√©
        return documentsPath.appendingPathComponent("custom_beep.wav")
    }
    
    /// G√©n√®re un son bip sinuso√Ødal simple
    /// @return URL du fichier audio g√©n√©r√©
    private func createBeepSound() -> URL {
        let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        let audioFilename = documentsPath.appendingPathComponent("interaction_beep.wav")
        
        if FileManager.default.fileExists(atPath: audioFilename.path) {
            return audioFilename
        }
        
        let sampleRate: Double = 44100
        let duration: Double = 0.3
        let frequency: Double = 880.0
        let amplitude: Float = 0.5
        
        let frameCount = UInt32(sampleRate * duration)
        let bytesPerFrame = 2
        let totalBytes = frameCount * UInt32(bytesPerFrame)
        
        var audioData = Data(count: Int(totalBytes))
        
        audioData.withUnsafeMutableBytes { (bytes: UnsafeMutableRawBufferPointer) in
            let samples = bytes.bindMemory(to: Int16.self)
            let frameCountInt = Int(frameCount)
            
            for i in 0..<frameCountInt {
                let timeValue = Double(i) / sampleRate
                let sinValue = sin(2.0 * Double.pi * frequency * timeValue)
                let scaledValue = amplitude * 32767.0 * Float(sinValue)
                let sample = Int16(scaledValue)
                samples[i] = sample
            }
        }
        
        try? audioData.write(to: audioFilename)
        return audioFilename
    }
    
    /// Demande l'autorisation d'acc√®s au microphone
    private func requestSpeechPermission() {
        SFSpeechRecognizer.requestAuthorization { [weak self] authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    self?.interactionEnabled = self?.speechAvailable ?? false
                case .denied, .restricted, .notDetermined:
                    self?.interactionEnabled = false
                @unknown default:
                    self?.interactionEnabled = false
                }
            }
        }
    }
    
    // MARK: - Interface Publique
    
    /// Connecte le gestionnaire de synth√®se vocale
    /// @param manager Instance de VoiceSynthesisManager
    func setVoiceSynthesisManager(_ manager: VoiceSynthesisManager) {
        self.voiceSynthesisManager = manager
    }
    
    /// Met √† jour la liste des objets importants d√©tect√©s
    /// @param objects Liste des objets avec leurs scores d'importance
    func updateImportantObjects(_ objects: [(object: TrackedObject, score: Float)]) {
        self.currentImportantObjects = objects
        
        // Si on est en mode collection pour travers√©e
        if isCollectingObjects {
            let now = Date()
            print("üì¶ COLLECTION - Nouveaux objets re√ßus: \(objects.count)")
            
            // Ajouter les nouveaux objets avec timestamp
            for item in objects {
                collectedObjects.append((object: item.object, score: item.score, timestamp: now))
                print("   + Ajout√©: \(item.object.label) (score: \(item.score))")
            }
            
            print("üìä COLLECTION - Total collect√©: \(collectedObjects.count) objets")
            
            // Pendant la collection, on ne fait PAS de r√©ponse anticip√©e ni d'autres annonces
            // On attend juste que le timer se termine
            print("üîí COLLECTION - Mode gel√©, pas d'annonces suppl√©mentaires")
            
        } else {
            // Pas en mode collection, juste une mise √† jour normale
            if objects.count > 0 {
                print("üì± UPDATE NORMAL - \(objects.count) objets d√©tect√©s")
            }
        }
    }
    
    /// V√©rifie le support de la reconnaissance locale
    /// @return true si la reconnaissance locale est support√©e
    func isLocalRecognitionSupported() -> Bool {
        if #available(iOS 13.0, *) {
            return speechRecognizer?.supportsOnDeviceRecognition ?? false
        }
        return false
    }
    
    /// Retourne la version iOS minimum requise
    /// @return String de la version iOS minimale
    func getMinimumIOSVersion() -> String {
        return "iOS 13.0+"
    }
    
    /// Active le mode √©coute (pr√©paration seulement)
    func startContinuousListening() {
        guard interactionEnabled && speechAvailable else { return }
        currentRetryCount = 0
        lastRecognizedText = "Pr√™t - appui long pour parler"
    }
    
    /// D√©sactive le mode √©coute
    func stopContinuousListening() {
        stopListening()
        resetCollection() // Nettoyer le syst√®me de collection
        isRecovering = false
        recoveryTimer?.invalidate()
        recoveryTimer = nil
        lastRecognizedText = ""
    }
    
    /// D√©marre l'√©coute pour UNE question (appel√© par appui long)
    func startSingleQuestion() {
        guard interactionEnabled && speechAvailable else {
            voiceSynthesisManager?.speak("Interaction vocale non disponible")
            return
        }
        
        guard !isListening else { return }
        
        // Arr√™t total de la synth√®se vocale pour lib√©rer l'audio
        voiceSynthesisManager?.stopSpeaking()
        voiceSynthesisManager?.interruptForInteraction(reason: "Question utilisateur")
        
        // Attente lib√©ration audio puis d√©marrage √©coute
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
            self?.playBeep()
            self?.isWaitingForQuestion = true
            
            // D√©lai fixe court pour r√©activit√© imm√©diate (son de 1s max)
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.4) { [weak self] in
                self?.startListening()
                
                // Timeout d'urgence uniquement (Apple g√®re la finalisation)
                self?.questionTimer = Timer.scheduledTimer(withTimeInterval: self?.emergencyTimeoutDuration ?? 30.0, repeats: false) { _ in
                    self?.handleEmergencyTimeout()
                }
            }
        }
    }
    
    // MARK: - M√©thodes d'Interaction avec Synth√®se Vocale
    
    /// Interrompt la synth√®se pour interaction
    /// @param reason Raison de l'interruption
    func interruptForInteraction(reason: String = "Interaction utilisateur") {
        voiceSynthesisManager?.interruptForInteraction(reason: reason)
    }
    
    /// Fait parler une r√©ponse d'interaction
    /// @param text Texte √† vocaliser
    /// @param priority Priorit√© du message
    func speakInteraction(_ text: String, priority: Int = 15) {
        voiceSynthesisManager?.speakInteraction(text)
    }
    
    /// Reprend la synth√®se normale apr√®s interaction
    func resumeAfterInteraction() {
        voiceSynthesisManager?.resumeAfterInteraction()
    }
    
    // MARK: - Reconnaissance Vocale Core
    
    /// D√©marre l'√©coute avec reconnaissance Apple
    private func startListening() {
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            handleSpeechError(NSError(domain: "SpeechRecognizer", code: -1, userInfo: [NSLocalizedDescriptionKey: "Service non disponible"]))
            return
        }
        
        // V√©rification p√©riode de r√©cup√©ration
        if let lastError = lastErrorTime, Date().timeIntervalSince(lastError) < retryDelay {
            scheduleRetry()
            return
        }
        
        stopListening()
        
        // Activation session audio
        do {
            try AVAudioSession.sharedInstance().setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            handleSpeechError(error)
            return
        }
        
        isListening = true
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            handleSpeechError(NSError(domain: "SpeechRecognizer", code: -2, userInfo: [NSLocalizedDescriptionKey: "Impossible de cr√©er la requ√™te"]))
            return
        }
        
        recognitionRequest.shouldReportPartialResults = true
        
        // For√ßage mode local obligatoire pour confidentialit√©
        if #available(iOS 13.0, *) {
            recognitionRequest.requiresOnDeviceRecognition = true
            
            if !speechRecognizer.supportsOnDeviceRecognition {
                handleSpeechError(NSError(domain: "SpeechRecognizer", code: -3, userInfo: [NSLocalizedDescriptionKey: "Mode local requis non support√©"]))
                return
            }
        } else {
            handleSpeechError(NSError(domain: "SpeechRecognizer", code: -4, userInfo: [NSLocalizedDescriptionKey: "iOS 13+ requis"]))
            return
        }
        
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        inputNode.removeTap(onBus: 0)
        
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
            recognitionRequest.append(buffer)
        }
        
        audioEngine.prepare()
        
        do {
            try audioEngine.start()
        } catch {
            handleSpeechError(error)
            return
        }
        
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            self?.handleRecognitionResult(result: result, error: error)
        }
    }
    
    /// Traite les r√©sultats de reconnaissance vocale Apple
    /// @param result R√©sultat de SFSpeechRecognitionTask
    /// @param error Erreur √©ventuelle
    private func handleRecognitionResult(result: SFSpeechRecognitionResult?, error: Error?) {
        if let error = error {
            handleSingleQuestionError(error)
            return
        }
        
        guard let result = result else { return }
        
        let recognizedText = result.bestTranscription.formattedString.lowercased()
        
        // Reset compteurs d'erreur en cas de succ√®s
        if !recognizedText.isEmpty {
            currentRetryCount = 0
            lastErrorTime = nil
        }
        
        DispatchQueue.main.async { [weak self] in
            self?.lastRecognizedText = recognizedText
            
            if result.isFinal {
                self?.processQuestion(text: recognizedText, isFinal: true)
            } else {
                // R√©sultat partiel - programmer timeout de finalisation
                self?.lastPartialUpdate = Date()
                self?.lastPartialResult = recognizedText
                self?.schedulePartialResultTimeout()
            }
        }
    }
    
    /// Programme un timeout pour finaliser les r√©sultats partiels
    private func schedulePartialResultTimeout() {
        partialResultTimer?.invalidate()
        
        partialResultTimer = Timer.scheduledTimer(withTimeInterval: 1.5, repeats: false) { [weak self] _ in
            guard let self = self else { return }
            
            if let lastUpdate = self.lastPartialUpdate,
               Date().timeIntervalSince(lastUpdate) >= 1.5,
               !self.lastPartialResult.isEmpty {
                
                self.processQuestion(text: self.lastPartialResult, isFinal: true)
            }
        }
    }
    
    /// G√®re les erreurs lors d'une question ponctuelle
    /// @param error Erreur de reconnaissance
    private func handleSingleQuestionError(_ error: Error) {
        if !lastPartialResult.isEmpty {
            return // Ignorer l'erreur si on a d√©j√† un r√©sultat partiel
        }
        
        stopListening()
        isWaitingForQuestion = false
        
        // Message selon le type d'erreur
        if error.localizedDescription.contains("No speech detected") {
            voiceSynthesisManager?.speak("Je n'ai rien entendu")
        } else {
            voiceSynthesisManager?.speak("Erreur de reconnaissance vocale")
        }
        
        // Reprendre synth√®se normale apr√®s erreur
        DispatchQueue.main.asyncAfter(deadline: .now() + 2.0) { [weak self] in
            self?.voiceSynthesisManager?.resumeAfterInteraction()
        }
        
        lastRecognizedText = "Erreur - appui long pour r√©essayer"
    }
    
    /// G√®re le timeout d'urgence (30s)
    private func handleEmergencyTimeout() {
        voiceSynthesisManager?.speak("Timeout d'urgence")
        finishSingleQuestion()
    }
    
    /// G√®re les erreurs de reconnaissance et retry automatique
    /// @param error Erreur de reconnaissance
    private func handleSpeechError(_ error: Error) {
        lastErrorTime = Date()
        currentRetryCount += 1
        
        stopListening()
        
        // Arr√™t temporaire si trop d'erreurs
        if currentRetryCount >= maxRetryAttempts {
            isRecovering = true
            
            recoveryTimer = Timer.scheduledTimer(withTimeInterval: retryDelay * 3, repeats: false) { [weak self] _ in
                self?.attemptRecovery()
            }
        } else {
            scheduleRetry()
        }
    }
    
    /// Programme une nouvelle tentative apr√®s d√©lai
    private func scheduleRetry() {
        DispatchQueue.main.asyncAfter(deadline: .now() + retryDelay) { [weak self] in
            if self?.interactionEnabled == true && self?.isRecovering != true {
                self?.startListening()
            }
        }
    }
    
    /// Tente de r√©cup√©rer le service vocal apr√®s erreurs multiples
    private func attemptRecovery() {
        isRecovering = false
        currentRetryCount = 0
        lastErrorTime = nil
        
        checkSpeechAvailability()
        
        if interactionEnabled && speechAvailable {
            DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) { [weak self] in
                // Service r√©cup√©r√©, pr√™t pour nouvelle question
                self?.lastRecognizedText = "Service r√©cup√©r√© - appui long pour parler"
            }
        }
    }
    
    /// Joue le son d'activation avec routage audio intelligent
    private func playBeep() {
        let audioSession = AVAudioSession.sharedInstance()
        let hasAirPods = audioSession.currentRoute.outputs.contains {
            $0.portType == .bluetoothA2DP || $0.portType == .bluetoothHFP
        }
        
        do {
            if hasAirPods {
                try audioSession.overrideOutputAudioPort(.none) // AirPods
            } else {
                try audioSession.overrideOutputAudioPort(.speaker) // Haut-parleurs
            }
        } catch {
            print("‚ùå Erreur route audio beep: \(error)")
        }
        
        beepPlayer?.stop()
        beepPlayer?.currentTime = 0
        beepPlayer?.play()
    }
    
    // MARK: - Syst√®me de Collection d'Objets sur 3 Secondes
    
    /// D√©marre une collection diff√©r√©e apr√®s lib√©ration de l'audio
    /// @param question Question qui n√©cessite la collection (travers√©e uniquement)
    private func startDelayedCollection(for question: ParsedQuestion) {
        print("üîÑ D√âBUT COLLECTION DIFF√âR√âE - Question: \(question.type), Objet: \(question.targetObject ?? "aucun")")
        
        // S'assurer que l'audio est compl√®tement lib√©r√©
        guard !isListening && !isWaitingForQuestion else {
            print("‚ùå Audio pas encore lib√©r√©, report de la collection")
            DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) { [weak self] in
                self?.startDelayedCollection(for: question)
            }
            return
        }
        
        print("   Objets actuels avant collection: \(currentImportantObjects.count)")
        
        isCollectingObjects = true
        pendingQuestion = question
        collectionStartTime = Date()
        collectedObjects.removeAll()
        
        // GELER les annonces pendant la collection
        voiceSynthesisManager?.interruptForInteraction(reason: "Collection en cours")
        
        // Message de collection puis d√©marrage imm√©diat du timer
        voiceSynthesisManager?.speak("Balayez la sc√®ne...")
        
        // D√©marrer le timer de collection imm√©diatement (pas d'attente)
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.5) { [weak self] in
            self?.collectionTimer = Timer.scheduledTimer(withTimeInterval: 3.0, repeats: false) { [weak self] _ in
                print("‚è∞ TIMER COLLECTION DIFF√âR√âE - 3 secondes √©coul√©es, finalisation...")
                self?.completeCollection()
            }
            
            print("‚è±Ô∏è Timer de collection diff√©r√©e d√©marr√© pour 3 secondes")
        }
    }
    
    /// V√©rifie si on peut r√©pondre imm√©diatement (signalisation + danger)
    /// @return true si conditions remplies pour r√©ponse imm√©diate
    private func canRespondImmediately() -> Bool {
        let allObjects = collectedObjects + currentImportantObjects.map { (object: $0.object, score: $0.score, timestamp: Date()) }
        
        var hasSignalization = false
        var hasDanger = false
        
        for item in allObjects {
            let label = item.object.label
            
            // Signalisation d√©tect√©e
            if ["traffic_light", "crosswalk", "traffic_sign"].contains(label) {
                hasSignalization = true
            }
            
            // Danger d√©tect√©
            if ["car", "truck", "bus", "motorcycle"].contains(label) {
                hasDanger = true
            }
        }
        
        return hasSignalization && hasDanger
    }
    
    /// Termine la collection anticip√©e et r√©pond imm√©diatement
    /// @param question Question en attente
    private func completeCollectionEarly(for question: ParsedQuestion) {
        print("‚ö° COMPLETE COLLECTION EARLY - Finalisation anticip√©e")
        
        collectionTimer?.invalidate()
        collectionTimer = nil
        
        let response = generateResponseFromCollection(for: question)
        print("üí¨ R√âPONSE ANTICIP√âE: '\(response)'")
        voiceSynthesisManager?.speak(response)
        
        resetCollection()
        
        print("‚úÖ Collection anticip√©e termin√©e")
    }
    
    /// Termine la collection apr√®s 3 secondes et g√©n√®re la r√©ponse
    private func completeCollection() {
        guard let question = pendingQuestion else {
            print("‚ùå COMPLETE COLLECTION - Pas de question en attente")
            return
        }
        
        print("üèÅ COMPLETE COLLECTION - Finalisation normale apr√®s 3 secondes")
        print("   Question type: \(question.type)")
        print("   Objet cible: \(question.targetObject ?? "aucun")")
        print("   Objets collect√©s: \(collectedObjects.count)")
        print("   Objets actuels: \(currentImportantObjects.count)")
        
        let response = generateResponseFromCollection(for: question)
        print("üí¨ R√âPONSE G√âN√âR√âE: '\(response)'")
        voiceSynthesisManager?.speak(response)
        
        resetCollection()
        
        // Finaliser la session apr√®s la r√©ponse
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
            print("üîö Finalisation de la session question")
            self?.finishSingleQuestion()
        }
    }
    
    /// G√©n√®re une r√©ponse bas√©e sur tous les objets collect√©s
    /// @param question Question √† traiter
    /// @return R√©ponse bas√©e sur la collection compl√®te
    private func generateResponseFromCollection(for question: ParsedQuestion) -> String {
        print("üîÄ G√âN√âRATION R√âPONSE COLLECTION")
        print("   Objets collect√©s: \(collectedObjects.count)")
        print("   Objets actuels: \(currentImportantObjects.count)")
        
        // Fusionner objets collect√©s + objets actuels
        let allObjectsWithScore = collectedObjects.map { (object: $0.object, score: $0.score) } + currentImportantObjects
        print("   Total fusionn√©: \(allObjectsWithScore.count) objets")
        
        // Log des objets fusionn√©s
        for (index, item) in allObjectsWithScore.enumerated() {
            print("     [\(index)] \(item.object.label) (score: \(item.score))")
        }
        
        // Cr√©er une analyse temporaire avec tous les objets
        let tempImportantObjects = currentImportantObjects
        self.currentImportantObjects = allObjectsWithScore
        
        let response: String
        switch question.type {
        case .crossing:
            print("   ‚Üí Traitement CROSSING avec collection")
            response = handleCrossingQuestion(analysis: analyzeCurrentScene())
        default:
            print("   ‚Üí Traitement DEFAULT avec collection")
            response = generateResponse(for: question)
        }
        
        // Restaurer objets originaux
        self.currentImportantObjects = tempImportantObjects
        print("   Objets restaur√©s: \(self.currentImportantObjects.count)")
        
        return response
    }
    
    /// Remet √† z√©ro le syst√®me de collection
    private func resetCollection() {
        print("üßπ RESET COLLECTION")
        print("   Collection active: \(isCollectingObjects)")
        print("   Objets collect√©s avant reset: \(collectedObjects.count)")
        print("   Timer actif: \(collectionTimer != nil)")
        
        isCollectingObjects = false
        pendingQuestion = nil
        collectionStartTime = nil
        collectedObjects.removeAll()
        collectionTimer?.invalidate()
        collectionTimer = nil
        
        print("   ‚úÖ Collection reset√©e")
    }
    
    // MARK: - Analyse et Traitement des Questions
    
    /// Traite une question finalis√©e et g√©n√®re la r√©ponse
    /// @param text Texte de la question
    /// @param isFinal true si finalisation confirm√©e
    private func processQuestion(text: String, isFinal: Bool) {
        partialResultTimer?.invalidate()
        partialResultTimer = nil
        questionTimer?.invalidate()
        questionTimer = nil
        
        print("üé§ PROCESSING QUESTION: '\(text)'")
        
        let parsedQuestion = parseQuestion(text)
        
        print("üìù QUESTION PARS√âE:")
        print("   Type final: \(parsedQuestion.type)")
        print("   Objet cible: \(parsedQuestion.targetObject ?? "aucun")")
        print("   Confiance: \(parsedQuestion.confidence)")
        print("   Objets disponibles: \(currentImportantObjects.count)")
        
        // V√©rifier si c'est une question de travers√©e qui n√©cessite collection
        if parsedQuestion.type == .crossing {
            print("üîÑ Question de travers√©e d√©tect√©e - Collection n√©cessaire")
            print("   ‚Üí PAS de r√©ponse imm√©diate, collection directe")
            
            // Toujours finaliser la session de reconnaissance en premier
            finishSingleQuestion()
            
            // D√©marrer la collection diff√©r√©e APR√àS avoir lib√©r√© l'audio
            DispatchQueue.main.asyncAfter(deadline: .now() + 1.5) { [weak self] in
                print("üîÑ D√©marrage collection pour travers√©e...")
                self?.startDelayedCollection(for: parsedQuestion)
            }
            return
        }
        
        // Pour toutes les autres questions (y compris localisation), r√©ponse imm√©diate normale
        let immediateResponse: String
        if currentImportantObjects.isEmpty {
            immediateResponse = "Je ne d√©tecte aucun objet actuellement"
        } else {
            immediateResponse = generateResponse(for: parsedQuestion)
        }
        
        print("üí¨ R√©ponse imm√©diate pour type \(parsedQuestion.type): '\(immediateResponse)'")
        voiceSynthesisManager?.speak(immediateResponse)
        
        // Finaliser la session
        finishSingleQuestion()
    }
    
    /// Finalise une session de question
    private func finishSingleQuestion() {
        print("üèÅ FINISH SINGLE QUESTION")
        print("   Collection en cours: \(isCollectingObjects)")
        print("   Listening: \(isListening)")
        
        questionTimer?.invalidate()
        questionTimer = nil
        isWaitingForQuestion = false
        
        // Si une collection est en cours, ne pas finaliser maintenant
        if isCollectingObjects {
            print("   ‚è≥ Collection en cours, ne pas finaliser maintenant")
            return
        }
        
        print("   üîá Arr√™t de l'√©coute")
        stopListening()
        
        // Reprendre synth√®se normale
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) { [weak self] in
            print("   üîä Reprise synth√®se normale")
            self?.voiceSynthesisManager?.resumeAfterInteraction()
        }
        
        lastRecognizedText = "Question trait√©e - appui long pour une nouvelle question"
        print("   ‚úÖ Session termin√©e")
    }
    
    
    /// Arr√™te compl√®tement l'√©coute et nettoie les ressources
    private func stopListening() {
        partialResultTimer?.invalidate()
        partialResultTimer = nil
        
        if audioEngine.isRunning {
            audioEngine.stop()
        }
        
        audioEngine.inputNode.removeTap(onBus: 0)
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionRequest = nil
        recognitionTask = nil
        isListening = false
        
        questionTimer?.invalidate()
        questionTimer = nil
        
        // D√©sactivation propre de la session audio
        do {
            try AVAudioSession.sharedInstance().setActive(false, options: .notifyOthersOnDeactivation)
        } catch {
            // Ignorer les erreurs de d√©sactivation
        }
    }
    
    // MARK: - Analyse S√©mantique des Questions
    
    /// Parse une question en fran√ßais et identifie son type et cible
    /// @param text Texte de la question
    /// @return ParsedQuestion avec type, objet cible et confiance
    private func parseQuestion(_ text: String) -> ParsedQuestion {
        let normalizedText = text.lowercased()
            .replacingOccurrences(of: "?", with: "")
            .replacingOccurrences(of: ".", with: "")
        
        print("üîç DEBUG PARSING - Texte original: '\(text)'")
        print("üîç DEBUG PARSING - Texte normalis√©: '\(normalizedText)'")
        
        var questionType: QuestionType = .unknown
        var confidence: Float = 0.0
        var detectedKeyword = ""
        
        // Test prioritaire des mots-cl√©s CROSSING (s√©curit√©)
        let crossingKeywords = getCrossingKeywords()
        print("üîç DEBUG PARSING - Test CROSSING avec mots-cl√©s: \(crossingKeywords)")
        for keyword in crossingKeywords {
            if normalizedText.contains(keyword) {
                questionType = .crossing
                confidence = 0.9
                detectedKeyword = keyword
                print("‚úÖ DEBUG PARSING - CROSSING d√©tect√© avec '\(keyword)'")
                break
            }
        }
        
        // Test des mots-cl√©s LOCATION (tr√®s sp√©cifique)
        if questionType == .unknown {
            let locationKeywords = getLocationKeywords()
            print("üîç DEBUG PARSING - Test LOCATION avec mots-cl√©s: \(locationKeywords)")
            for keyword in locationKeywords {
                if normalizedText.contains(keyword) {
                    questionType = .location
                    confidence = 0.9
                    detectedKeyword = keyword
                    print("‚úÖ DEBUG PARSING - LOCATION d√©tect√© avec '\(keyword)'")
                    break
                }
            }
        }
        
        // Test des mots-cl√©s COUNT (plus sp√©cifique)
        if questionType == .unknown {
            let countKeywords = getCountKeywords()
            print("üîç DEBUG PARSING - Test COUNT avec mots-cl√©s: \(countKeywords)")
            for keyword in countKeywords {
                if normalizedText.contains(keyword) {
                    questionType = .count
                    confidence = 0.8
                    detectedKeyword = keyword
                    print("‚úÖ DEBUG PARSING - COUNT d√©tect√© avec '\(keyword)'")
                    break
                }
            }
        }
        
        // Test des autres types de questions
        if questionType == .unknown {
            let questionKeywords = getQuestionKeywords()
            print("üîç DEBUG PARSING - Test autres types de questions")
            for (type, keywords) in questionKeywords {
                if type == .crossing || type == .count || type == .location { continue }
                print("üîç DEBUG PARSING - Test \(type) avec mots-cl√©s: \(keywords)")
                for keyword in keywords {
                    if normalizedText.contains(keyword) {
                        questionType = type
                        confidence = 0.8
                        detectedKeyword = keyword
                        print("‚úÖ DEBUG PARSING - \(type) d√©tect√© avec '\(keyword)'")
                        break
                    }
                }
                if confidence > 0 { break }
            }
        }
        
        var targetObject: String?
        
        // Reconnaissance d'objet avec dictionnaire complet
        print("üîç DEBUG PARSING - Recherche d'objet cible dans le dictionnaire...")
        for (englishObject, frenchVariants) in objectTranslations {
            for variant in frenchVariants {
                if normalizedText.contains(variant) {
                    targetObject = englishObject
                    confidence = max(confidence, 0.7)
                    print("‚úÖ DEBUG PARSING - Objet trouv√©: '\(variant)' -> '\(englishObject)'")
                    break
                }
            }
            if targetObject != nil { break }
        }
        
        // Cas sp√©ciaux pour am√©liorer la d√©tection
        if normalizedText.contains("sc√®ne") || normalizedText.contains("situation") {
            questionType = .sceneOverview
            confidence = 0.9
            detectedKeyword = "sc√®ne/situation"
            print("‚úÖ DEBUG PARSING - SCENE_OVERVIEW d√©tect√© avec mot sp√©cial")
        }
        
        if normalizedText.contains("devant") {
            questionType = .description
            confidence = max(confidence, 0.8)
            detectedKeyword = "devant"
            print("‚úÖ DEBUG PARSING - DESCRIPTION d√©tect√© avec 'devant'")
        }
        
        // AJOUT: D√©tection sp√©ciale pour "position" sans mot-cl√© "o√π"
        if normalizedText.contains("position") && questionType == .unknown {
            questionType = .location
            confidence = 0.8
            detectedKeyword = "position"
            print("‚úÖ DEBUG PARSING - LOCATION d√©tect√© avec 'position' (cas sp√©cial)")
        }
        
        let result = ParsedQuestion(
            type: questionType,
            targetObject: targetObject,
            confidence: confidence,
            originalText: text
        )
        
        print("üéØ DEBUG PARSING - R√âSULTAT:")
        print("   Type: \(questionType)")
        print("   Objet cible: \(targetObject ?? "aucun")")
        print("   Confiance: \(confidence)")
        print("   Mot-cl√© d√©tect√©: '\(detectedKeyword)'")
        print("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        
        return result
    }
    
    // MARK: - G√©n√©ration des R√©ponses Contextuelles
    
    /// G√©n√®re une r√©ponse adapt√©e au type de question et au contexte
    /// @param question Question pars√©e avec type et cible
    /// @return String de r√©ponse √† vocaliser
    private func generateResponse(for question: ParsedQuestion) -> String {
        let analysis = analyzeCurrentScene()
        
        print("üéØ G√âN√âRATION R√âPONSE pour type: \(question.type)")
        print("   Objets analys√©s: \(analysis.totalObjects)")
        print("   Objet cible demand√©: \(question.targetObject ?? "aucun")")
        
        switch question.type {
        case .presence:
            print("   ‚Üí Appel handlePresenceQuestion")
            return handlePresenceQuestion(question, analysis: analysis)
        case .count:
            print("   ‚Üí Appel handleCountQuestion")
            return handleCountQuestion(question, analysis: analysis)
        case .location:
            print("   ‚Üí Appel handleLocationQuestion")
            return handleLocationQuestion(question, analysis: analysis)
        case .description:
            print("   ‚Üí Appel handleDescriptionQuestion")
            return handleDescriptionQuestion(analysis: analysis)
        case .sceneOverview:
            print("   ‚Üí Appel handleSceneOverviewQuestion")
            return handleSceneOverviewQuestion(analysis: analysis)
        case .crossing:
            print("   ‚Üí Appel handleCrossingQuestion")
            return handleCrossingQuestion(analysis: analysis)
        case .specific:
            print("   ‚Üí Appel handleSpecificQuestion")
            return handleSpecificQuestion(question, analysis: analysis)
        case .unknown:
            print("   ‚Üí Appel handleUnknownQuestion")
            return handleUnknownQuestion(question, analysis: analysis)
        }
    }
    
    /// Analyse la sc√®ne actuelle et structure les informations
    /// @return SceneAnalysis avec objets organis√©s par type/zone/distance
    private func analyzeCurrentScene() -> SceneAnalysis {
        var objectsByType: [String: Int] = [:]
        var objectsByZone: [String: [String]] = [
            "devant": [], "gauche": [], "droite": []
        ]
        var distances: [String: Float] = [:]
        var criticalObjects: [String] = []
        var navigationObjects: [String] = []
        
        for (_, item) in currentImportantObjects.enumerated() {
            let object = item.object
            let frenchLabel = translateToFrench(object.label)
            
            // Comptage par type
            objectsByType[frenchLabel, default: 0] += 1
            
            // Analyse de position spatiale
            let zone = getZoneFromBoundingBox(object.lastRect)
            objectsByZone[zone, default: []].append(frenchLabel)
            
            // Stockage des distances
            if let distance = object.distance {
                let key = "\(frenchLabel)_\(object.trackingNumber)"
                distances[key] = distance
            }
            
            // Classification en objets critiques
            if item.score > 0.7 {
                criticalObjects.append(frenchLabel)
            }
            
            // Classification en objets de navigation
            if ["traffic_light", "traffic_sign", "crosswalk", "street_light", "traffic_cone"].contains(object.label) {
                navigationObjects.append(frenchLabel)
            }
        }
        
        return SceneAnalysis(
            totalObjects: currentImportantObjects.count,
            objectsByType: objectsByType,
            objectsByZone: objectsByZone,
            distances: distances,
            criticalObjects: criticalObjects,
            navigationObjects: navigationObjects
        )
    }
    
    /// Traite les questions de pr√©sence ("Y a-t-il une voiture ?")
    /// @param question Question pars√©e
    /// @param analysis Analyse de sc√®ne
    /// @return R√©ponse textuelle
    private func handlePresenceQuestion(_ question: ParsedQuestion, analysis: SceneAnalysis) -> String {
        guard let targetObject = question.targetObject else {
            if analysis.totalObjects == 0 {
                return "Non, aucun objet d√©tect√© actuellement"
            }
            
            let objectsList = analysis.objectsByType.map { type, count in
                if count == 1 {
                    return "une \(type)"
                } else {
                    return "\(count) \(type)s"
                }
            }
            
            if objectsList.count == 1 {
                return "Oui, je vois \(objectsList[0])"
            } else if objectsList.count == 2 {
                return "Oui, je vois \(objectsList[0]) et \(objectsList[1])"
            } else {
                let allButLast = objectsList.dropLast().joined(separator: ", ")
                return "Oui, je vois \(allButLast) et \(objectsList.last!)"
            }
        }
        
        let frenchLabel = translateToFrench(targetObject)
        let count = analysis.objectsByType[frenchLabel] ?? 0
        
        if count > 0 {
            return count == 1 ?
                "Oui je vois une \(frenchLabel)" :
                "Oui je vois \(count) \(frenchLabel)s"
        } else {
            return "Non je n'en vois pas"
        }
    }
    
    /// Traite les questions de comptage ("Combien de voitures ?")
    /// @param question Question pars√©e
    /// @param analysis Analyse de sc√®ne
    /// @return R√©ponse textuelle
    private func handleCountQuestion(_ question: ParsedQuestion, analysis: SceneAnalysis) -> String {
        guard let targetObject = question.targetObject else {
            let total = analysis.totalObjects
            if total == 0 {
                return "Je ne d√©tecte aucun objet actuellement"
            } else if total == 1 {
                return "Je d√©tecte un objet"
            } else {
                let summaryParts = analysis.objectsByType.map { type, count in
                    "\(count) \(type)\(count > 1 ? "s" : "")"
                }
                let summary = summaryParts.joined(separator: ", ")
                return "Je d√©tecte \(total) objets au total : \(summary)"
            }
        }
        
        let frenchLabel = translateToFrench(targetObject)
        let count = analysis.objectsByType[frenchLabel] ?? 0
        
        switch count {
        case 0:
            return "Aucune \(frenchLabel) d√©tect√©e"
        case 1:
            return "Une \(frenchLabel)"
        default:
            return "\(count) \(frenchLabel)s"
        }
    }
    
    /// Traite les questions de localisation ("O√π est la voiture ?")
    /// @param question Question pars√©e
    /// @param analysis Analyse de sc√®ne
    /// @return R√©ponse textuelle
    private func handleLocationQuestion(_ question: ParsedQuestion, analysis: SceneAnalysis) -> String {
        print("üìç HANDLE LOCATION QUESTION")
        print("   Objet cible: \(question.targetObject ?? "aucun")")
        print("   Objets analys√©s: \(analysis.totalObjects)")
        print("   Types d'objets: \(analysis.objectsByType)")
        
        guard let targetObject = question.targetObject else {
            print("   ‚Üí Pas d'objet cible, r√©ponse g√©n√©rale")
            return generateGeneralLocationResponse(analysis: analysis)
        }
        
        let frenchLabel = translateToFrench(targetObject)
        let count = analysis.objectsByType[frenchLabel] ?? 0
        
        print("   Label fran√ßais: '\(frenchLabel)'")
        print("   Nombre trouv√©: \(count)")
        
        if count == 0 {
            return "Je ne vois pas de \(frenchLabel) actuellement"
        }
        
        let location = findObjectLocation(targetObject, in: analysis)
        let distance = findObjectDistance(targetObject, in: analysis)
        
        var response = "La \(frenchLabel) est \(location.isEmpty ? "visible" : location)"
        if !distance.isEmpty {
            response += " \(distance)"
        }
        
        if count > 1 {
            response = "Je vois \(count) \(frenchLabel)s. " + response.replacingOccurrences(of: "La \(frenchLabel)", with: "L'une d'elles")
        }
        
        print("   R√©ponse g√©n√©r√©e: '\(response)'")
        return response
    }
    
    /// Traite les questions de description ("Qu'est-ce qui est devant moi ?")
    /// @param analysis Analyse de sc√®ne
    /// @return R√©ponse textuelle
    private func handleDescriptionQuestion(analysis: SceneAnalysis) -> String {
        if analysis.totalObjects == 0 {
            return "Je ne vois rien de particulier devant vous"
        }
        
        let frontObjects = analysis.objectsByZone["devant"] ?? []
        
        if frontObjects.isEmpty {
            return "Rien directement devant vous, mais je d√©tecte des objets sur les c√¥t√©s"
        }
        
        let objectGroups = Dictionary(grouping: frontObjects) { $0 }
        let objectList = objectGroups.map { obj, occurrences in
            let count = occurrences.count
            return count > 1 ? "\(count) \(obj)s" : "une \(obj)"
        }.joined(separator: ", ")
        
        return "Devant vous, je vois : \(objectList)"
    }
    
    /// Traite les questions de vue d'ensemble ("D√©cris la sc√®ne")
    /// @param analysis Analyse de sc√®ne
    /// @return R√©ponse textuelle structur√©e par plans
    private func handleSceneOverviewQuestion(analysis: SceneAnalysis) -> String {
        if analysis.totalObjects == 0 {
            return "Aucun objet d√©tect√©"
        }
        
        let plansAnalysis = analyzeSceneByPlans()
        var description: [String] = []
        
        // Description par plans de profondeur avec formulation naturelle
        if !plansAnalysis.firstPlan.isEmpty {
            let firstPlanDesc = createGroupedDescription(plansAnalysis.firstPlan)
            description.append("Au premier plan, je vois \(firstPlanDesc)")
        }
        
        if !plansAnalysis.secondPlan.isEmpty {
            let secondPlanDesc = createGroupedDescription(plansAnalysis.secondPlan)
            description.append("Plus loin, \(secondPlanDesc)")
        }
        
        if !plansAnalysis.thirdPlan.isEmpty {
            let thirdPlanDesc = createGroupedDescription(plansAnalysis.thirdPlan)
            description.append("Au fond, \(thirdPlanDesc)")
        }
        
        // Information d'ambiance
        let ambiance = determineAmbiance(analysis)
        if !ambiance.isEmpty {
            description.append(ambiance)
        }
        
        let result = description.isEmpty ?
            "\(analysis.totalObjects) objet\(analysis.totalObjects > 1 ? "s" : "") d√©tect√©\(analysis.totalObjects > 1 ? "s" : "")" :
            description.joined(separator: ". ")
        
        return result
    }
    
    // MARK: - Analyse Sp√©cialis√©e Aide √† la Travers√©e
    
    /// Traite les questions de travers√©e ("Puis-je traverser ?")
    /// @param analysis Analyse de sc√®ne
    /// @return Conseil de s√©curit√© adaptatif
    private func handleCrossingQuestion(analysis: SceneAnalysis) -> String {
        // Analyse signalisation de travers√©e
        let crossingSignalization = analyzeCrossingSignalization(analysis)
        
        // Analyse situation circulation
        let trafficAnalysis = analyzeTrafficSituation(analysis)
        
        // G√©n√©ration conseil s√©curit√©
        return generateCrossingAdvice(signalization: crossingSignalization, traffic: trafficAnalysis)
    }
    
    private struct CrossingSignalization {
        let hasTrafficLight: Bool
        let hasCrosswalk: Bool
        let hasTrafficSigns: Bool
        let hasStreetLight: Bool
        let signalizationScore: Int
    }
    
    private struct TrafficSituation {
        let vehicleCount: Int
        let closeVehicles: Int
        let movingVehicles: Int
        let safetyScore: Int
    }
    
    /// Analyse la signalisation disponible pour la travers√©e
    /// @param analysis Analyse de sc√®ne g√©n√©rale
    /// @return CrossingSignalization avec √©l√©ments d√©tect√©s
    private func analyzeCrossingSignalization(_ analysis: SceneAnalysis) -> CrossingSignalization {
        var hasTrafficLight = false
        var hasCrosswalk = false
        var hasTrafficSigns = false
        var hasStreetLight = false
        
        for (object, _) in currentImportantObjects {
            switch object.label {
            case "traffic_light":
                hasTrafficLight = true
            case "crosswalk":
                hasCrosswalk = true
            case "traffic_sign":
                hasTrafficSigns = true
            case "street_light":
                hasStreetLight = true
            default:
                break
            }
        }
        
        // Score de signalisation (0-4)
        var score = 0
        if hasTrafficLight { score += 2 }
        if hasCrosswalk { score += 2 }
        if hasTrafficSigns { score += 1 }
        if hasStreetLight { score += 1 }
        
        return CrossingSignalization(
            hasTrafficLight: hasTrafficLight,
            hasCrosswalk: hasCrosswalk,
            hasTrafficSigns: hasTrafficSigns,
            hasStreetLight: hasStreetLight,
            signalizationScore: score
        )
    }
    
    /// Analyse la situation de circulation
    /// @param analysis Analyse de sc√®ne g√©n√©rale
    /// @return TrafficSituation avec m√©trics de s√©curit√©
    private func analyzeTrafficSituation(_ analysis: SceneAnalysis) -> TrafficSituation {
        let vehicleTypes = ["voiture", "camion", "bus", "moto", "v√©lo"]
        var vehicleCount = 0
        var closeVehicles = 0
        var movingVehicles = 0
        
        for (object, score) in currentImportantObjects {
            let frenchLabel = translateToFrench(object.label)
            
            if vehicleTypes.contains(frenchLabel) {
                vehicleCount += 1
                
                // V√©hicules proches (< 5m)
                if let distance = object.distance, distance < 5.0 {
                    closeVehicles += 1
                }
                
                // V√©hicules en mouvement (score √©lev√©)
                if score > 0.8 {
                    movingVehicles += 1
                }
            }
        }
        
        // Score de s√©curit√© (0-10)
        var safetyScore = 10
        safetyScore -= closeVehicles * 3
        safetyScore -= movingVehicles * 2
        safetyScore -= max(0, vehicleCount - 2)
        safetyScore = max(0, safetyScore)
        
        return TrafficSituation(
            vehicleCount: vehicleCount,
            closeVehicles: closeVehicles,
            movingVehicles: movingVehicles,
            safetyScore: safetyScore
        )
    }
    
    /// G√©n√®re un conseil de travers√©e adaptatif
    /// @param signalization Analyse signalisation
    /// @param traffic Analyse circulation
    /// @return Conseil personnalis√©
    private func generateCrossingAdvice(signalization: CrossingSignalization, traffic: TrafficSituation) -> String {
        // Signalisation compl√®te (feu + passage pi√©ton)
        if signalization.hasTrafficLight && signalization.hasCrosswalk {
            if traffic.safetyScore >= 7 {
                return "Oui, signalisation compl√®te pr√©sente. Traversez au feu vert avec prudence"
            } else if traffic.closeVehicles > 0 {
                return "Signalisation pr√©sente mais circulation dense. Attendez que les v√©hicules passent"
            } else {
                return "Signalisation pr√©sente. V√©rifiez le feu et traversez prudemment"
            }
        }
        
        // Passage pi√©ton sans feu
        if signalization.hasCrosswalk && !signalization.hasTrafficLight {
            if traffic.safetyScore >= 8 {
                return "Passage pi√©ton d√©tect√©, circulation calme. Vous pouvez traverser prudemment"
            } else if traffic.closeVehicles > 2 {
                return "Passage pi√©ton pr√©sent mais circulation dense. Attendez une accalmie"
            } else {
                return "Passage pi√©ton pr√©sent. V√©rifiez bien la circulation avant de traverser"
            }
        }
        
        // Feu sans passage pi√©ton visible
        if signalization.hasTrafficLight && !signalization.hasCrosswalk {
            return "Feu de circulation d√©tect√©. Cherchez le passage pi√©ton √† proximit√©"
        }
        
        // Signalisation minimale ou absente
        if signalization.signalizationScore <= 1 {
            if traffic.vehicleCount == 0 {
                return "Aucune signalisation et aucun v√©hicule"
            } else if traffic.safetyScore >= 8 {
                return "Pas de signalisation officielle. Circulation calme mais restez tr√®s prudent"
            } else {
                return "Pas de signalisation s√©curis√©e et circulation pr√©sente. Cherchez un passage am√©nag√©"
            }
        }
        
        // Signalisation partielle
        if traffic.safetyScore >= 6 {
            return "Signalisation partielle, circulation mod√©r√©e. Travers√©e possible avec grande prudence"
        } else {
            return "Signalisation insuffisante et circulation dense. Trouvez un passage plus s√ªr"
        }
    }
    
    /// Traite les questions sp√©cifiques (d√©l√©gu√© vers localisation)
    /// @param question Question pars√©e
    /// @param analysis Analyse de sc√®ne
    /// @return R√©ponse textuelle
    private func handleSpecificQuestion(_ question: ParsedQuestion, analysis: SceneAnalysis) -> String {
        return handleLocationQuestion(question, analysis: analysis)
    }
    
    /// Traite les questions non reconnues avec aide contextuelle
    /// @param question Question pars√©e
    /// @param analysis Analyse de sc√®ne
    /// @return R√©ponse avec aide et contexte
    private func handleUnknownQuestion(_ question: ParsedQuestion, analysis: SceneAnalysis) -> String {
        print("‚ùì QUESTION UNKNOWN - Analyse:")
        print("   Texte original: '\(question.originalText)'")
        print("   Type d√©tect√©: \(question.type)")
        print("   Objet cible: \(question.targetObject ?? "aucun")")
        print("   Confiance: \(question.confidence)")
        
        let text = question.originalText.lowercased()
        
        if text.contains("aide") || text.contains("help") {
            return "Vous pouvez me demander s'il y a des objets, combien il y en a, o√π ils sont, si vous pouvez traverser, ou me demander de d√©crire la sc√®ne"
        }
        
        if analysis.totalObjects == 0 {
            return "Je ne d√©tecte aucun objet actuellement. Reformulez votre question si besoin"
        }
        
        let mainObjectsList = Array(analysis.objectsByType.prefix(3))
        let mainObjects = mainObjectsList.map { type, count in
            "\(count) \(type)\(count > 1 ? "s" : "")"
        }.joined(separator: ", ")
        
        return "Je ne suis pas s√ªr de comprendre. Actuellement je vois : \(mainObjects)"
    }
    
    // MARK: - Analyse par Plans de Profondeur
    
    private struct PlansAnalysis {
        let firstPlan: [ObjectDescription]      // < 3m (distance critique)
        let secondPlan: [ObjectDescription]     // 3-6m
        let thirdPlan: [ObjectDescription]      // > 6m
    }
    
    private struct ObjectDescription {
        let frenchName: String
        let distance: Float?
        let zone: String
        let count: Int
        let isCritical: Bool
        let isNavigation: Bool
    }
    
    /// Analyse la sc√®ne par plans de profondeur
    /// @return PlansAnalysis structur√©e par distance
    private func analyzeSceneByPlans() -> PlansAnalysis {
        var firstPlan: [ObjectDescription] = []
        var secondPlan: [ObjectDescription] = []
        var thirdPlan: [ObjectDescription] = []
        
        // Groupement par type d'objet
        let objectGroups = Dictionary(grouping: currentImportantObjects) {
            translateToFrench($0.object.label)
        }
        
        for (frenchName, objects) in objectGroups {
            let distances = objects.compactMap { $0.object.distance }
            let avgDistance = distances.isEmpty ? nil : distances.reduce(0, +) / Float(distances.count)
            let minDistance = distances.min()
            
            // Zone principale
            let zones = objects.map { getZoneFromBoundingBox($0.object.lastRect) }
            let zoneCounts = Dictionary(grouping: zones, by: { $0 }).mapValues { $0.count }
            let mainZone = zoneCounts.max(by: { $0.value < $1.value })?.key ?? "devant"
            
            // Classification criticit√© et navigation
            let isCritical = objects.contains { $0.score > 0.7 }
            let isNavigation = objects.contains {
                ["traffic_light", "traffic_sign", "crosswalk", "street_light", "traffic_cone"].contains($0.object.label)
            }
            
            let objectDesc = ObjectDescription(
                frenchName: frenchName,
                distance: avgDistance,
                zone: mainZone,
                count: objects.count,
                isCritical: isCritical,
                isNavigation: isNavigation
            )
            
            // Classification par plan selon distance
            if let minDist = minDistance {
                if minDist < 3.0 {
                    firstPlan.append(objectDesc)
                } else if minDist < 6.0 {
                    secondPlan.append(objectDesc)
                } else {
                    thirdPlan.append(objectDesc)
                }
            } else {
                // Sans distance, classer par priorit√©
                if isCritical {
                    firstPlan.append(objectDesc)
                } else {
                    secondPlan.append(objectDesc)
                }
            }
        }
        
        return PlansAnalysis(
            firstPlan: firstPlan.sorted { $0.distance ?? 0 < $1.distance ?? 0 },
            secondPlan: secondPlan.sorted { $0.distance ?? 5 < $1.distance ?? 5 },
            thirdPlan: thirdPlan.sorted { $0.distance ?? 10 < $1.distance ?? 10 }
        )
    }
    
    /// Cr√©e une description group√©e pour un plan de profondeur
    /// @param objects Liste d'objets du plan
    /// @return Description textuelle naturelle et group√©e
    private func createGroupedDescription(_ objects: [ObjectDescription]) -> String {
        if objects.isEmpty { return "" }
        
        var descriptions: [String] = []
        
        for obj in objects {
            let objectName = getGroupedObjectName(obj)
            let locationInfo = getLocationInfo(obj)
            
            let fullDescription = locationInfo.isEmpty ? objectName : "\(objectName)\(locationInfo)"
            descriptions.append(fullDescription)
        }
        
        return joinDescriptions(descriptions)
    }
    
    /// Obtient le nom d'un objet avec quantit√© pour description group√©e
    /// @param obj Description d'objet
    /// @return Nom avec quantit√© appropri√©e
    private func getGroupedObjectName(_ obj: ObjectDescription) -> String {
        if obj.count == 1 {
            return getDirectName(obj)
        } else {
            return "\(obj.count) \(obj.frenchName)s"
        }
    }
    
    /// Obtient l'information de localisation simplifi√©e
    /// @param obj Description d'objet
    /// @return Information de localisation
    private func getLocationInfo(_ obj: ObjectDescription) -> String {
        var location = ""
        
        // Position spatiale uniquement
        switch obj.zone {
        case "gauche":
            location = " √† gauche"
        case "droite":
            location = " √† droite"
        case "devant":
            location = ""  // Pas besoin de pr√©ciser "devant" dans une description narrative
        default:
            location = ""
        }
        
        return location
    }
    
    /// Obtient le nom direct d'un objet en fran√ßais naturel
    /// @param obj Description d'objet
    /// @return Nom avec article appropri√©
    private func getDirectName(_ obj: ObjectDescription) -> String {
        let directNames: [String: String] = [
            "voiture": "une voiture",
            "personne": "une personne",
            "arbre": "un arbre",
            "b√¢timent": "un b√¢timent",
            "poteau": "un poteau",
            "feu de circulation": "un feu",
            "panneau de signalisation": "un panneau",
            "trottoir": "le trottoir",
            "route": "la route",
            "passage pi√©ton": "un passage pi√©ton",
            "lampadaire": "un lampadaire",
            "banc": "un banc",
            "v√©g√©tation": "de la v√©g√©tation",
            "mur": "un mur",
            "eau": "de l'eau"
        ]
        
        return directNames[obj.frenchName] ?? "une \(obj.frenchName)"
    }
    
    /// Joint les descriptions avec conjonctions appropri√©es
    /// @param descriptions Liste de descriptions
    /// @return Texte joint naturellement
    private func joinDescriptions(_ descriptions: [String]) -> String {
        if descriptions.isEmpty { return "" }
        if descriptions.count == 1 { return descriptions[0] }
        if descriptions.count == 2 { return "\(descriptions[0]) et \(descriptions[1])" }
        
        let allButLast = descriptions.dropLast().joined(separator: ", ")
        return "\(allButLast) et \(descriptions.last!)"
    }
    
    /// D√©termine l'ambiance g√©n√©rale de la sc√®ne
    /// @param analysis Analyse de sc√®ne
    /// @return Description d'ambiance
    private func determineAmbiance(_ analysis: SceneAnalysis) -> String {
        if analysis.criticalObjects.count > 2 {
            return "Attention, plusieurs objets proches"
        }
        
        if analysis.navigationObjects.count > 0 {
            return "Signalisation pr√©sente"
        }
        
        let vehicleCount = analysis.objectsByType.filter {
            ["voiture", "camion", "bus", "moto"].contains($0.key)
        }.values.reduce(0, +)
        
        if vehicleCount > 3 {
            return "Circulation dense"
        }
        
        if analysis.totalObjects < 3 {
            return "Environnement calme"
        }
        
        return ""
    }
    
    // MARK: - M√©thodes Utilitaires
    
    /// Traduit un label anglais vers fran√ßais
    /// @param englishLabel Label anglais du mod√®le
    /// @return √âquivalent fran√ßais
    private func translateToFrench(_ englishLabel: String) -> String {
        for (english, frenchVariants) in objectTranslations {
            if english == englishLabel {
                return frenchVariants.first ?? englishLabel
            }
        }
        return englishLabel
    }
    
    /// D√©termine la zone spatiale d'une bounding box
    /// @param rect Rectangle de d√©tection
    /// @return Zone spatiale ("gauche", "droite", "devant")
    private func getZoneFromBoundingBox(_ rect: CGRect) -> String {
        let centerX = rect.midX
        
        if centerX < 0.3 {
            return "gauche"
        } else if centerX > 0.7 {
            return "droite"
        } else {
            return "devant"
        }
    }
    
    /// Trouve la localisation d'un objet dans l'analyse
    /// @param objectType Type d'objet recherch√©
    /// @param analysis Analyse de sc√®ne
    /// @return Description de localisation
    private func findObjectLocation(_ objectType: String, in analysis: SceneAnalysis) -> String {
        let frenchLabel = translateToFrench(objectType)
        
        for (zone, objects) in analysis.objectsByZone {
            if objects.contains(frenchLabel) {
                return "√† \(zone == "devant" ? "votre avant" : zone)"
            }
        }
        return ""
    }
    
    /// Trouve la distance d'un objet dans l'analyse
    /// @param objectType Type d'objet recherch√©
    /// @param analysis Analyse de sc√®ne
    /// @return Description de distance
    private func findObjectDistance(_ objectType: String, in analysis: SceneAnalysis) -> String {
        let frenchLabel = translateToFrench(objectType)
        var closestDistance: Float?
        
        for (key, distance) in analysis.distances {
            if key.contains(frenchLabel) {
                if closestDistance == nil || distance < closestDistance! {
                    closestDistance = distance
                }
            }
        }
        
        guard let distance = closestDistance else { return "" }
        
        if distance < 1.0 {
            return "√† \(Int(distance * 100)) centim√®tres"
        } else if distance < 10.0 {
            return "√† \(String(format: "%.1f", distance)) m√®tres"
        } else {
            return "√† \(Int(distance)) m√®tres"
        }
    }
    
    /// G√©n√®re une r√©ponse de localisation g√©n√©rale
    /// @param analysis Analyse de sc√®ne
    /// @return Description g√©n√©rale des localisations
    private func generateGeneralLocationResponse(analysis: SceneAnalysis) -> String {
        if analysis.totalObjects == 0 {
            return "Aucun objet d√©tect√© pour localiser"
        }
        
        var locations: [String] = []
        
        for (zone, objects) in analysis.objectsByZone {
            if !objects.isEmpty {
                let count = objects.count
                let zoneDescription = zone == "devant" ? "devant vous" : "√† \(zone)"
                locations.append("\(count) objet\(count > 1 ? "s" : "") \(zoneDescription)")
            }
        }
        
        return locations.isEmpty ? "Objets d√©tect√©s sans position pr√©cise" : locations.joined(separator: ", ")
    }
    
    // MARK: - Interface Publique pour Statistiques
    
    /// Retourne les statistiques compl√®tes du gestionnaire
    /// @return String format√© avec toutes les informations
    func getStats() -> String {
        let statusText: String
        
        if isRecovering {
            statusText = "üîÑ R√©cup√©ration en cours"
        } else if isListening {
            statusText = isWaitingForQuestion ? "üé§ √âcoute d'une question" : "üëÇ √âcoute active"
        } else if interactionEnabled {
            statusText = "‚úÖ Pr√™t (touchez le micro pour parler)"
        } else {
            statusText = "‚ùå Non disponible"
        }
        
        let errorInfo = currentRetryCount > 0 ? "\n‚ö†Ô∏è Erreurs r√©centes: \(currentRetryCount)/\(maxRetryAttempts)" : ""
        
        var privacyInfo = "üîí Mode: LOCAL uniquement (100% priv√©)"
        if #available(iOS 13.0, *) {
            if let speechRecognizer = speechRecognizer {
                if !speechRecognizer.supportsOnDeviceRecognition {
                    privacyInfo = "‚ùå Mode local non support√© (interaction d√©sactiv√©e)"
                }
            }
        } else {
            privacyInfo = "‚ùå iOS 13+ requis pour mode local"
        }
        
        var soundInfo = "üéµ Son d'activation: "
        if let soundURL = beepPlayer?.url {
            if soundURL.lastPathComponent.contains("custom") ||
               soundURL.lastPathComponent.contains("ecoute") ||
               soundURL.lastPathComponent.contains("activation") {
                soundInfo += "Personnalis√© (\(soundURL.lastPathComponent))"
            } else {
                soundInfo += "Bip g√©n√©r√©"
            }
        } else {
            soundInfo += "Non charg√©"
        }
        
        return """
        üé§ Interaction Vocale - AIDE √Ä LA TRAVERS√âE + DICTIONNAIRE COMPLET:
           - √âtat: \(statusText)
           - Service: \(speechAvailable ? "‚úÖ Disponible" : "‚ùå Indisponible")
           - \(privacyInfo)
           - \(soundInfo)
           - Derni√®re activit√©: "\(lastRecognizedText)"
           - Objets analys√©s: \(currentImportantObjects.count)
           - Dictionnaire d'objets: \(objectTranslations.count) types support√©s\(errorInfo)

        üö¶ ANALYSE INTELLIGENTE DE TRAVERS√âE:
           ‚Ä¢ √âvaluation automatique de la signalisation
           ‚Ä¢ Score de s√©curit√© bas√© sur la circulation
           ‚Ä¢ Conseils adaptatifs selon la situation

        üé® DESCRIPTION PAR PLANS:
           ‚Ä¢ Premier plan (< 3m distance critique), Plus loin (3-6m), Au fond (> 6m)
           ‚Ä¢ Description naturelle et group√©e
           ‚Ä¢ Informations spatiales pr√©cises

        üí° Mode d'emploi:
           1. Appui long sur l'√©cran (0.8s)
           2. Attendez le son d'activation üéµ
           3. Posez votre question clairement
           4. Apple g√®re automatiquement la finalisation

        üîí Confidentialit√© garantie:
           - Aucune donn√©e audio envoy√©e sur internet
           - Traitement 100% local sur votre appareil
           - Pas d'√©coute continue (√©conomie batterie)
        """
    }
    
    /// Indique si le service est pr√™t pour une question
    /// @return true si pr√™t √† traiter une question
    func isReadyForQuestion() -> Bool {
        return interactionEnabled && speechAvailable && !isListening && !isRecovering
    }
}
