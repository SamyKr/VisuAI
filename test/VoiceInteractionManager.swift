import Foundation
import AVFoundation
import Speech
import UIKit

// MARK: - Enums et Structures
enum VoiceCommand: String, CaseIterable {
    case activate = "dis moi"
    case alternative1 = "dis-moi"
    case alternative2 = "√©coute"
    case alternative3 = "hey"

    static let activationPhrases = VoiceCommand.allCases.map { $0.rawValue }
}

enum QuestionType {
    case presence      // "Y a-t-il une voiture ?"
    case count         // "Combien de voitures ?"
    case location      // "O√π est la voiture ?"
    case description   // "Qu'est-ce qui est devant moi ?"
    case sceneOverview // "D√©cris la sc√®ne"
    case specific      // "O√π est le feu ?"
    case unknown
}

struct ParsedQuestion {
    let type: QuestionType
    let targetObject: String?
    let confidence: Float
    let originalText: String
}

struct SceneAnalysis {
    let totalObjects: Int
    let objectsByType: [String: Int]
    let objectsByZone: [String: [String]]
    let distances: [String: Float]
    let criticalObjects: [String]
    let navigationObjects: [String]
}

// MARK: - VoiceInteractionManager avec confiance totale en Apple
class VoiceInteractionManager: NSObject, ObservableObject {

    // MARK: - Configuration
    private let listeningTimeout: TimeInterval = 5.0
    private let activationTimeout: TimeInterval = 2.0
    private let emergencyTimeoutDuration: TimeInterval = 30.0  // Timeout d'urgence seulement
    private let maxRetryAttempts = 3
    private let retryDelay: TimeInterval = 2.0

    // üîí RECONNAISSANCE LOCALE UNIQUEMENT - Aucune donn√©e envoy√©e sur internet

    // MARK: - √âtat
    @Published var isListening = false
    @Published var isWaitingForQuestion = false
    @Published var lastRecognizedText = ""
    @Published var interactionEnabled = true

    // MARK: - Gestion d'erreurs
    private var currentRetryCount = 0
    private var lastErrorTime: Date?
    private var isRecovering = false
    private var speechAvailable = true

    // MARK: - Reconnaissance vocale
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "fr-FR"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()

    // MARK: - Audio
    private var beepPlayer: AVAudioPlayer?
    private var questionTimer: Timer?
    private var activationTimer: Timer?
    private var recoveryTimer: Timer?

    // MARK: - R√©solution du probl√®me isFinal
    private var lastPartialUpdate: Date?
    private var lastPartialResult: String = ""
    private var partialResultTimer: Timer?

    // MARK: - R√©f√©rences externes
    weak var voiceSynthesisManager: VoiceSynthesisManager?
    private var currentImportantObjects: [(object: TrackedObject, score: Float)] = []

    // MARK: - Dictionnaire de traduction invers√©e (√©tendu)
    private let objectTranslations: [String: [String]] = [
        "person": ["personne", "personnes", "gens", "pi√©ton", "pi√©tons", "homme", "femme", "enfant"],
        "car": ["voiture", "voitures", "auto", "autos", "v√©hicule", "v√©hicules", "bagnole", "caisse"],
        "truck": ["camion", "camions", "poids lourd", "poids lourds", "semi", "semi-remorque"],
        "bus": ["bus", "autobus", "car"],
        "motorcycle": ["moto", "motos", "motocyclette", "motocyclettes", "scooter", "scooters"],
        "bicycle": ["v√©lo", "v√©los", "bicyclette", "bicyclettes", "bike"],
        "traffic light": ["feu", "feux", "feu de circulation", "feux de circulation", "feu tricolore", "signal"],
        "traffic sign": ["panneau", "panneaux", "panneau de signalisation", "signalisation", "stop"],
        "pedestrian crossing": ["passage pi√©ton", "passage pi√©tons", "passage clout√©", "zebra"],
        "pole": ["poteau", "poteaux", "pilier", "piliers", "m√¢t"],
        "curb": ["bordure", "bordures", "trottoir", "trottoirs"],
        "road": ["route", "routes", "rue", "rues", "chauss√©e", "voie"],
        "building": ["b√¢timent", "b√¢timents", "immeuble", "immeubles", "maison", "maisons"],
        "tree": ["arbre", "arbres"],
        "light": ["lumi√®re", "lumi√®res", "√©clairage", "lampe", "lampes"]
    ]

    // MARK: - Mots-cl√©s pour les questions
    private func getPresenceKeywords() -> [String] {
        return ["y a-t-il", "ya-t-il", "est-ce qu'il y a", "il y a", "vois-tu", "d√©tectes-tu","tu vois"]
    }

    private func getCountKeywords() -> [String] {
        return ["combien", "nombre", "quantit√©"]
    }

    private func getLocationKeywords() -> [String] {
        return ["o√π", "ou", "position", "situ√©e", "situ√©", "place"]
    }

    private func getDescriptionKeywords() -> [String] {
        return ["qu'est-ce qui", "que vois-tu", "devant moi", "autour"]
    }

    private func getSceneKeywords() -> [String] {
        return ["d√©cris", "d√©cris la sc√®ne", "que se passe-t-il", "situation"]
    }

    private func getQuestionKeywords() -> [QuestionType: [String]] {
        return [
            .presence: getPresenceKeywords(),
            .count: getCountKeywords(),
            .location: getLocationKeywords(),
            .description: getDescriptionKeywords(),
            .sceneOverview: getSceneKeywords()
        ]
    }

    override init() {
        super.init()
        setupAudio()
        requestSpeechPermission()
        setupBeepSound()
        checkSpeechAvailability()
        print("üé§ VoiceInteractionManager initialis√© - Fait confiance au syst√®me Apple")
    }

    deinit {
        cleanupResources()
    }

    // MARK: - Configuration et nettoyage

    private func setupAudio() {
        do {
            let audioSession = AVAudioSession.sharedInstance()
            
            // ‚úÖ NOUVELLE CONFIGURATION : Support AirPods + Bluetooth
            try audioSession.setCategory(
                .playAndRecord,
                mode: .measurement,
                options: [
                    .duckOthers,
                    .allowBluetooth,           // ‚Üê NOUVEAU : Autoriser Bluetooth
                    .allowBluetoothA2DP,       // ‚Üê NOUVEAU : Autoriser AirPods/casques
                    .allowAirPlay              // ‚Üê NOUVEAU : Autoriser AirPlay
                    // ‚ùå SUPPRIM√â : .defaultToSpeaker (for√ßait le haut-parleur)
                ]
            )
            
            try audioSession.setActive(false) // Commencer inactif
            
            // ‚úÖ NOUVEAU : Forcer la route intelligente D√àS LE D√âBUT
            let hasAirPods = audioSession.currentRoute.outputs.contains {
                $0.portType == .bluetoothA2DP || $0.portType == .bluetoothHFP
            }
            
            if hasAirPods {
                try audioSession.overrideOutputAudioPort(.none) // AirPods
                print("‚úÖ Route forc√©e vers AirPods d√®s l'init")
            } else {
                try audioSession.overrideOutputAudioPort(.speaker) // Haut-parleurs
                print("üîä Route forc√©e vers haut-parleurs d√®s l'init")
            }
            
            print("‚úÖ Configuration audio r√©ussie - AirPods support√©s")
            
            // üîç Debug : Afficher les routes audio disponibles
            printAvailableAudioRoutes()
            
        } catch {
            print("‚ùå Erreur configuration audio: \(error)")
        }
    }
    
    // ‚úÖ NOUVELLE M√âTHODE : Debug des routes audio
    private func printAvailableAudioRoutes() {
        let audioSession = AVAudioSession.sharedInstance()
        
        print("üì± Routes audio disponibles :")
        for output in audioSession.availableInputs ?? [] {
            print("  - Entr√©e : \(output.portName) (\(output.portType.rawValue))")
        }
        
        print("üîä Route de sortie actuelle :")
        for output in audioSession.currentRoute.outputs {
            print("  - Sortie : \(output.portName) (\(output.portType.rawValue))")
        }
        
        if audioSession.currentRoute.outputs.contains(where: { $0.portType == .bluetoothA2DP || $0.portType == .bluetoothHFP }) {
            print("‚úÖ AirPods/Bluetooth d√©tect√©s et actifs")
        } else {
            print("‚ö†Ô∏è Pas d'AirPods d√©tect√©s - v√©rifiez la connexion")
        }
    }

    private func checkSpeechAvailability() {
        guard let speechRecognizer = speechRecognizer else {
            speechAvailable = false
            interactionEnabled = false
            print("‚ùå SFSpeechRecognizer non initialis√©")
            return
        }

        speechAvailable = speechRecognizer.isAvailable

        if !speechAvailable {
            print("‚ùå Reconnaissance vocale non disponible")
            interactionEnabled = false
            return
        }

        // V√©rifier le support de la reconnaissance locale OBLIGATOIRE
        if #available(iOS 13.0, *) {
            if speechRecognizer.supportsOnDeviceRecognition {
                print("‚úÖ Reconnaissance vocale LOCALE pr√™te (100% priv√©, hors ligne)")
                interactionEnabled = true
            } else {
                print("‚ùå Reconnaissance locale non support√©e sur cet appareil")
                print("   L'interaction vocale sera d√©sactiv√©e pour pr√©server la vie priv√©e")
                interactionEnabled = false
            }
        } else {
            print("‚ùå iOS < 13.0 - reconnaissance locale requise non disponible")
            print("   Mise √† jour iOS recommand√©e pour l'interaction vocale")
            interactionEnabled = false
        }
    }

    private func cleanupResources() {
        stopListening()
        recoveryTimer?.invalidate()
        recoveryTimer = nil

        do {
            try AVAudioSession.sharedInstance().setActive(false)
        } catch {
            print("‚ùå Erreur d√©sactivation session audio: \(error)")
        }
    }

    private func setupBeepSound() {
        let beepURL = createBeepSound()
        do {
            beepPlayer = try AVAudioPlayer(contentsOf: beepURL)
            beepPlayer?.prepareToPlay()
            beepPlayer?.volume = 0.8
        } catch {
            print("‚ùå Erreur cr√©ation beep: \(error)")
        }
    }

    private func createBeepSound() -> URL {
        let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        let audioFilename = documentsPath.appendingPathComponent("interaction_beep.wav")

        if FileManager.default.fileExists(atPath: audioFilename.path) {
            return audioFilename
        }

        let sampleRate: Double = 44100
        let duration: Double = 0.3
        let frequency: Double = 880.0
        let amplitude: Float = 0.5

        let frameCount = UInt32(sampleRate * duration)
        let bytesPerFrame = 2
        let totalBytes = frameCount * UInt32(bytesPerFrame)

        var audioData = Data(count: Int(totalBytes))

        audioData.withUnsafeMutableBytes { (bytes: UnsafeMutableRawBufferPointer) in
            let samples = bytes.bindMemory(to: Int16.self)
            let frameCountInt = Int(frameCount)

            for i in 0..<frameCountInt {
                let timeValue = Double(i) / sampleRate
                let sinValue = sin(2.0 * Double.pi * frequency * timeValue)
                let scaledValue = amplitude * 32767.0 * Float(sinValue)
                let sample = Int16(scaledValue)
                samples[i] = sample
            }
        }

        try? audioData.write(to: audioFilename)
        return audioFilename
    }

    private func requestSpeechPermission() {
        SFSpeechRecognizer.requestAuthorization { [weak self] authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    print("‚úÖ Permission reconnaissance vocale accord√©e")
                    self?.interactionEnabled = self?.speechAvailable ?? false
                case .denied, .restricted, .notDetermined:
                    print("‚ùå Permission reconnaissance vocale refus√©e")
                    self?.interactionEnabled = false
                @unknown default:
                    self?.interactionEnabled = false
                }
            }
        }
    }

    // MARK: - Interface publique

    func setVoiceSynthesisManager(_ manager: VoiceSynthesisManager) {
        self.voiceSynthesisManager = manager
    }

    func updateImportantObjects(_ objects: [(object: TrackedObject, score: Float)]) {
        self.currentImportantObjects = objects
        print("üîÑ VoiceInteraction: Re√ßu \(objects.count) objets importants")
        for (index, item) in objects.enumerated() {
            print("   \(index + 1). #\(item.object.trackingNumber) \(item.object.label) (score: \(item.score))")
        }
    }

    /// V√©rifie si la reconnaissance locale est support√©e sur cet appareil
    func isLocalRecognitionSupported() -> Bool {
        if #available(iOS 13.0, *) {
            return speechRecognizer?.supportsOnDeviceRecognition ?? false
        }
        return false
    }

    /// Retourne la version iOS minimum requise pour l'interaction vocale
    func getMinimumIOSVersion() -> String {
        return "iOS 13.0+"
    }

    func startContinuousListening() {
        guard interactionEnabled && speechAvailable else {
            print("‚ùå Interaction vocale d√©sactiv√©e ou service indisponible")
            return
        }

        guard !isListening && !isRecovering else {
            print("‚ö†Ô∏è √âcoute d√©j√† active ou en r√©cup√©ration")
            return
        }

        print("üé§ Mode interaction vocale ACTIV√â (appuyez sur le bouton mic pour parler)")
        // Note: On n'√©coute PAS en continu, on attend l'activation manuelle
        currentRetryCount = 0
        lastRecognizedText = "Pr√™t - touchez le micro pour parler"
    }

    func stopContinuousListening() {
        stopListening()
        isRecovering = false
        recoveryTimer?.invalidate()
        recoveryTimer = nil
        lastRecognizedText = ""
        print("üé§ Mode interaction vocale D√âSACTIV√â")
    }

    /// D√©marre l'√©coute pour UNE question (appel√© par l'appui long)
    func startSingleQuestion() {
        guard interactionEnabled && speechAvailable else {
            voiceSynthesisManager?.speak("Interaction vocale non disponible")
            return
        }

        guard !isListening else {
            print("‚ö†Ô∏è √âcoute d√©j√† en cours")
            return
        }

        print("üé§ D√©marrage √©coute d'UNE question - Apple g√®re la finalisation")

        // üõë ARR√äT TOTAL DE TOUTE SYNTH√àSE VOCALE
        voiceSynthesisManager?.stopSpeaking()
        voiceSynthesisManager?.interruptForInteraction(reason: "Question utilisateur")

        // Attendre que l'audio se lib√®re compl√®tement
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
            // Jouer le beep
            self?.playBeep()

            // Passer en mode question directement
            self?.isWaitingForQuestion = true

            // D√©marrer l'√©coute apr√®s le beep avec plus de d√©lai
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.8) { [weak self] in
                self?.startListening(forActivation: false)

                // üéØ TIMEOUT BEAUCOUP PLUS LONG - Juste s√©curit√© contre les blocages
                self?.questionTimer = Timer.scheduledTimer(withTimeInterval: self?.emergencyTimeoutDuration ?? 30.0, repeats: false) { _ in
                    self?.handleEmergencyTimeout()
                }
            }
        }
    }

    func toggleListening() {
        if isListening {
            stopContinuousListening()
        } else {
            startContinuousListening()
        }
    }

    // MARK: - M√©thodes publiques pour interaction directe

    func interruptForInteraction(reason: String = "Interaction utilisateur") {
        print("üõë Interruption pour interaction: \(reason)")
        voiceSynthesisManager?.interruptForInteraction(reason: reason)
    }

    func speakInteraction(_ text: String, priority: Int = 15) {
        voiceSynthesisManager?.speakInteraction(text, priority: priority)
        print("üé§ Message d'interaction envoy√©: '\(text)'")
    }

    func resumeAfterInteraction() {
        voiceSynthesisManager?.resumeAfterInteraction()
        print("‚ñ∂Ô∏è Reprise apr√®s interaction")
    }

    // MARK: - Reconnaissance vocale avec confiance totale en Apple

    private func startListening(forActivation: Bool = false) {
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            print("‚ùå Reconnaissance vocale non disponible")
            handleSpeechError(NSError(domain: "SpeechRecognizer", code: -1, userInfo: [NSLocalizedDescriptionKey: "Service non disponible"]))
            return
        }

        // V√©rifier si on est en p√©riode de r√©cup√©ration
        if let lastError = lastErrorTime, Date().timeIntervalSince(lastError) < retryDelay {
            print("‚è≥ En attente avant nouvelle tentative...")
            scheduleRetry(forActivation: forActivation)
            return
        }

        stopListening() // Nettoyer toute session pr√©c√©dente

        // Activer la session audio
        do {
            try AVAudioSession.sharedInstance().setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            print("‚ùå Erreur activation session audio: \(error)")
            handleSpeechError(error)
            return
        }

        isListening = true

        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            handleSpeechError(NSError(domain: "SpeechRecognizer", code: -2, userInfo: [NSLocalizedDescriptionKey: "Impossible de cr√©er la requ√™te"]))
            return
        }

        // üîí RECONNAISSANCE LOCALE UNIQUEMENT üîí
        recognitionRequest.shouldReportPartialResults = true

        if #available(iOS 13.0, *) {
            // OBLIGATOIRE : Mode local seulement
            recognitionRequest.requiresOnDeviceRecognition = true
            print("‚úÖ Mode LOCAL forc√© (aucune donn√©e envoy√©e sur internet)")

            // Double v√©rification s√©curit√©
            if !speechRecognizer.supportsOnDeviceRecognition {
                print("‚ùå ERREUR : Mode local requis mais non support√©")
                handleSpeechError(NSError(domain: "SpeechRecognizer", code: -3, userInfo: [NSLocalizedDescriptionKey: "Mode local requis non support√©"]))
                return
            }
        } else {
            print("‚ùå iOS 13+ requis pour reconnaissance locale s√©curis√©e")
            handleSpeechError(NSError(domain: "SpeechRecognizer", code: -4, userInfo: [NSLocalizedDescriptionKey: "iOS 13+ requis"]))
            return
        }

        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)

        // Supprimer le tap existant s'il y en a un
        inputNode.removeTap(onBus: 0)

        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
            recognitionRequest.append(buffer)
        }

        audioEngine.prepare()

        do {
            try audioEngine.start()
            print("‚úÖ Moteur audio d√©marr√© - Apple g√®re la finalisation naturellement")
        } catch {
            print("‚ùå Erreur d√©marrage audio engine: \(error)")
            handleSpeechError(error)
            return
        }

        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            self?.handleRecognitionResult(result: result, error: error, forActivation: forActivation)
        }

        // üéØ PAS DE TIMER AGRESSIF - Laisser Apple g√©rer !
        // Seulement pour l'activation continue (si utilis√©)
        if forActivation {
            DispatchQueue.main.asyncAfter(deadline: .now() + activationTimeout) { [weak self] in
                if self?.isListening == true && self?.isWaitingForQuestion != true && self?.isRecovering != true {
                    self?.restartListening(forActivation: true)
                }
            }
        }
    }

    private func handleRecognitionResult(result: SFSpeechRecognitionResult?, error: Error?, forActivation: Bool) {
        if let error = error {
            print("‚ùå Erreur reconnaissance: \(error)")

            // Pour une question ponctuelle, on g√®re diff√©remment
            if !forActivation {
                handleSingleQuestionError(error)
            } else {
                handleSpeechError(error, forActivation: forActivation)
            }
            return
        }

        guard let result = result else { return }

        let recognizedText = result.bestTranscription.formattedString.lowercased()

        print("üçé Apple dit: '\(recognizedText)' (final: \(result.isFinal))")

        // Reset des compteurs d'erreur en cas de succ√®s
        if !recognizedText.isEmpty {
            currentRetryCount = 0
            lastErrorTime = nil
        }

        DispatchQueue.main.async { [weak self] in
            self?.lastRecognizedText = recognizedText

            if forActivation {
                self?.checkForActivation(text: recognizedText)
            } else {
                if result.isFinal {
                    // Cas normal - Apple a finalis√©
                    self?.processQuestion(text: recognizedText, isFinal: true)
                } else {
                    // R√©sultat partiel - mettre √† jour et d√©marrer un timer
                    self?.lastPartialUpdate = Date()
                    self?.lastPartialResult = recognizedText
                    self?.schedulePartialResultTimeout()
                }
            }
        }
    }

    private func schedulePartialResultTimeout() {
        // Annuler tout timer pr√©c√©dent
        partialResultTimer?.invalidate()

        // D√©marrer un nouveau timer
        partialResultTimer = Timer.scheduledTimer(withTimeInterval: 1.5, repeats: false) { [weak self] _ in
            guard let self = self else { return }

            // V√©rifier si nous avons re√ßu des mises √† jour depuis
            if let lastUpdate = self.lastPartialUpdate,
               Date().timeIntervalSince(lastUpdate) >= 1.5,
               !self.lastPartialResult.isEmpty {

                print("‚ÑπÔ∏è D√©tection de fin de phrase apr√®s timeout")
                self.processQuestion(text: self.lastPartialResult, isFinal: true)
            }
        }
    }

    private func handleSingleQuestionError(_ error: Error) {
        print("‚ùå Erreur lors de la question: \(error)")

        if !lastPartialResult.isEmpty {
                print("‚ÑπÔ∏è Ignorant l'erreur car nous avons d√©j√† un r√©sultat partiel: '\(lastPartialResult)'")
                return
            }
        // Pour les questions ponctuelles, on ne fait pas de retry
        stopListening()
        isWaitingForQuestion = false

        // Message selon le type d'erreur
        if error.localizedDescription.contains("No speech detected") {
            voiceSynthesisManager?.speak("Je n'ai rien entendu")
        } else {
            voiceSynthesisManager?.speak("Erreur de reconnaissance vocale")
        }

        // ‚ñ∂Ô∏è REPRENDRE LA SYNTH√àSE VOCALE NORMALE APR√àS L'ERREUR
        DispatchQueue.main.asyncAfter(deadline: .now() + 2.0) { [weak self] in
            self?.voiceSynthesisManager?.resumeAfterInteraction()
            print("‚ñ∂Ô∏è Synth√®se vocale normale reprise apr√®s erreur")
        }

        // Ne PAS relancer automatiquement
        lastRecognizedText = "Erreur - appui long pour r√©essayer"
    }

    // Timer d'urgence seulement (pas pour l'usage normal)
    private func handleEmergencyTimeout() {
        print("üö® TIMEOUT D'URGENCE (30s) - Quelque chose ne va pas")
        voiceSynthesisManager?.speak("Timeout d'urgence")
        finishSingleQuestion()
    }

    private func handleSpeechError(_ error: Error, forActivation: Bool = false) {
        lastErrorTime = Date()
        currentRetryCount += 1

        print("‚ùå Erreur Speech (tentative \(currentRetryCount)/\(maxRetryAttempts)): \(error)")

        // Nettoyer les ressources
        stopListening()

        // Si on d√©passe le nombre max de tentatives, arr√™ter temporairement
        if currentRetryCount >= maxRetryAttempts {
            print("üõë Trop d'erreurs, arr√™t temporaire du service vocal")
            isRecovering = true

            // Programmer une r√©cup√©ration apr√®s un d√©lai plus long
            recoveryTimer = Timer.scheduledTimer(withTimeInterval: retryDelay * 3, repeats: false) { [weak self] _ in
                self?.attemptRecovery()
            }
        } else {
            // Tentative de relance apr√®s un court d√©lai
            scheduleRetry(forActivation: forActivation)
        }
    }

    private func scheduleRetry(forActivation: Bool) {
        print("‚è≥ Programmation nouvelle tentative dans \(retryDelay)s...")
        DispatchQueue.main.asyncAfter(deadline: .now() + retryDelay) { [weak self] in
            if self?.interactionEnabled == true && self?.isRecovering != true {
                self?.startListening(forActivation: forActivation)
            }
        }
    }

    private func attemptRecovery() {
        print("üîÑ Tentative de r√©cup√©ration du service vocal...")
        isRecovering = false
        currentRetryCount = 0
        lastErrorTime = nil

        // V√©rifier la disponibilit√©
        checkSpeechAvailability()

        if interactionEnabled && speechAvailable {
            print("‚úÖ Service vocal r√©cup√©r√©, red√©marrage...")

            // Petit d√©lai avant de red√©marrer
            DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) { [weak self] in
                self?.startListening(forActivation: true)
            }
        } else {
            print("‚ùå Service vocal toujours indisponible")
        }
    }

    private func restartListening(forActivation: Bool) {
        print("üîÑ Red√©marrage √©coute...")
        stopListening()

        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
            self?.startListening(forActivation: forActivation)
        }
    }

    private func checkForActivation(text: String) {
        for phrase in VoiceCommand.activationPhrases {
            if text.contains(phrase) {
                print("üéØ Phrase d'activation d√©tect√©e: '\(phrase)'")
                handleActivation()
                return
            }
        }
    }

    private func handleActivation() {
        voiceSynthesisManager?.stopSpeaking()
        playBeep()
        isWaitingForQuestion = true

        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
            self?.startListening(forActivation: false)

            self?.questionTimer = Timer.scheduledTimer(withTimeInterval: self?.listeningTimeout ?? 5.0, repeats: false) { _ in
                self?.timeoutQuestion()
            }
        }
    }

    private func playBeep() {
        // ‚úÖ NOUVEAU : Logique audio intelligente (m√™me que VoiceSynthesisManager)
        let audioSession = AVAudioSession.sharedInstance()
        let hasAirPods = audioSession.currentRoute.outputs.contains {
            $0.portType == .bluetoothA2DP || $0.portType == .bluetoothHFP
        }
        
        do {
            if hasAirPods {
                try audioSession.overrideOutputAudioPort(.none) // AirPods
            } else {
                try audioSession.overrideOutputAudioPort(.speaker) // Haut-parleurs
            }
        } catch {
            print("‚ùå Erreur route audio beep: \(error)")
        }
        
        beepPlayer?.stop()
        beepPlayer?.currentTime = 0
        beepPlayer?.play()
        print("üîä Beep d'activation jou√©")
    }

    // Version modifi√©e pour g√©rer le timeout des r√©sultats partiels
    private func processQuestion(text: String, isFinal: Bool) {
        print("üìù processQuestion: '\(text)' (final: \(isFinal))")

        // Annuler le timer s'il existe
        partialResultTimer?.invalidate()
        partialResultTimer = nil

        // Annuler le timer d'urgence
        questionTimer?.invalidate()
        questionTimer = nil

        let parsedQuestion = parseQuestion(text)

        // Debug info
        let debugInfo = "Question analys√©e: Type \(parsedQuestion.type), Objet \(parsedQuestion.targetObject ?? "aucun"), \(currentImportantObjects.count) objets d√©tect√©s"
        print("üêõ \(debugInfo)")

        if !currentImportantObjects.isEmpty {
            let response = generateResponse(for: parsedQuestion)
            voiceSynthesisManager?.speak(response)
        } else {
            voiceSynthesisManager?.speak("Je ne d√©tecte aucun objet actuellement")
        }

        finishSingleQuestion()
    }

    private func finishSingleQuestion() {
        questionTimer?.invalidate()
        questionTimer = nil
        isWaitingForQuestion = false
        stopListening()

        // ‚ñ∂Ô∏è REPRENDRE LA SYNTH√àSE VOCALE NORMALE APR√àS LA QUESTION
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) { [weak self] in
            self?.voiceSynthesisManager?.resumeAfterInteraction()
            print("‚ñ∂Ô∏è Synth√®se vocale normale reprise apr√®s question")
        }

        // Message d'√©tat pour l'utilisateur
        lastRecognizedText = "Question trait√©e - appui long pour une nouvelle question"
        print("‚úÖ Question trait√©e, synth√®se normale va reprendre")
    }

    private func timeoutQuestion() {
        print("‚è∞ Timeout question - retour mode activation")
        voiceSynthesisManager?.speak("Je n'ai pas entendu de question")
        resetToActivationMode()
    }

    private func resetToActivationMode() {
        questionTimer?.invalidate()
        questionTimer = nil
        isWaitingForQuestion = false

        DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) { [weak self] in
            if self?.interactionEnabled == true && self?.isRecovering != true {
                self?.startListening(forActivation: true)
            }
        }
    }

    private func stopListening() {
        partialResultTimer?.invalidate()
        partialResultTimer = nil

        if audioEngine.isRunning {
            audioEngine.stop()
        }

        audioEngine.inputNode.removeTap(onBus: 0)
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionRequest = nil
        recognitionTask = nil
        isListening = false

        questionTimer?.invalidate()
        questionTimer = nil
        activationTimer?.invalidate()
        activationTimer = nil

        // D√©sactiver la session audio proprement
        do {
            try AVAudioSession.sharedInstance().setActive(false, options: .notifyOthersOnDeactivation)
        } catch {
            // Ignorer les erreurs de d√©sactivation - c'est normal
        }
    }

    // MARK: - Analyse des questions

    private func parseQuestion(_ text: String) -> ParsedQuestion {
        let normalizedText = text.lowercased()
            .replacingOccurrences(of: "?", with: "")
            .replacingOccurrences(of: ".", with: "")

        print("üîç PARSE: '\(text)' ‚Üí '\(normalizedText)'")

        var questionType: QuestionType = .unknown
        var confidence: Float = 0.0

        // Test des mots-cl√©s COUNT en premier (plus sp√©cifique)
        let countKeywords = getCountKeywords()
        for keyword in countKeywords {
            if normalizedText.contains(keyword) {
                questionType = .count
                confidence = 0.8
                print("‚úÖ PARSE: COUNT d√©tect√© avec '\(keyword)'")
                break
            }
        }

        // Si pas COUNT, tester les autres
        if questionType == .unknown {
            let questionKeywords = getQuestionKeywords()
            for (type, keywords) in questionKeywords {
                if type == .count { continue } // D√©j√† test√©
                for keyword in keywords {
                    if normalizedText.contains(keyword) {
                        questionType = type
                        confidence = 0.8
                        print("‚úÖ PARSE: \(type) d√©tect√© avec '\(keyword)'")
                        break
                    }
                }
                if confidence > 0 { break }
            }
        }

        var targetObject: String?

        // Test de reconnaissance d'objet
        for (englishObject, frenchVariants) in objectTranslations {
            for variant in frenchVariants {
                if normalizedText.contains(variant) {
                    targetObject = englishObject
                    confidence = max(confidence, 0.7)
                    print("‚úÖ PARSE: Objet '\(variant)' ‚Üí '\(englishObject)'")
                    break
                }
            }
            if targetObject != nil { break }
        }

        // Cas sp√©ciaux
        if normalizedText.contains("sc√®ne") || normalizedText.contains("situation") {
            questionType = .sceneOverview
            confidence = 0.9
            print("‚úÖ PARSE: sc√®ne/situation d√©tect√©")
        }

        if normalizedText.contains("devant") {
            questionType = .description
            confidence = max(confidence, 0.8)
            print("‚úÖ PARSE: devant d√©tect√©")
        }

        let result = ParsedQuestion(
            type: questionType,
            targetObject: targetObject,
            confidence: confidence,
            originalText: text
        )

        print("üìã PARSE FINAL: Type=\(questionType), Objet=\(targetObject ?? "aucun"), Conf=\(confidence)")

        return result
    }

    // MARK: - G√©n√©ration des r√©ponses

    private func generateResponse(for question: ParsedQuestion) -> String {
        print("üí¨ G√©n√©ration r√©ponse pour: '\(question.originalText)'")
        print("   - Type: \(question.type), Objet: \(question.targetObject ?? "aucun")")

        let analysis = analyzeCurrentScene()

        let response: String
        switch question.type {
        case .presence:
            response = handlePresenceQuestion(question, analysis: analysis)
        case .count:
            response = handleCountQuestion(question, analysis: analysis)
        case .location:
            response = handleLocationQuestion(question, analysis: analysis)
        case .description:
            response = handleDescriptionQuestion(analysis: analysis)
        case .sceneOverview:
            response = handleSceneOverviewQuestion(analysis: analysis)
        case .specific:
            response = handleSpecificQuestion(question, analysis: analysis)
        case .unknown:
            response = handleUnknownQuestion(question, analysis: analysis)
        }

        print("üí¨ R√©ponse: '\(response)'")
        return response
    }

    private func analyzeCurrentScene() -> SceneAnalysis {
        print("üìä Analyse de sc√®ne avec \(currentImportantObjects.count) objets importants")

        var objectsByType: [String: Int] = [:]
        var objectsByZone: [String: [String]] = [
            "devant": [], "gauche": [], "droite": []
        ]
        var distances: [String: Float] = [:]
        var criticalObjects: [String] = []
        var navigationObjects: [String] = []

        for (index, item) in currentImportantObjects.enumerated() {
            let object = item.object
            let frenchLabel = translateToFrench(object.label)

            print("   \(index + 1). #\(object.trackingNumber) \(object.label) ‚Üí \(frenchLabel) (score: \(item.score))")

            // Compter par type
            objectsByType[frenchLabel, default: 0] += 1

            // Analyser la position
            let zone = getZoneFromBoundingBox(object.lastRect)
            objectsByZone[zone, default: []].append(frenchLabel)

            // Distance
            if let distance = object.distance {
                let key = "\(frenchLabel)_\(object.trackingNumber)"
                distances[key] = distance
                print("     ‚Üí Distance: \(distance)m")
            }

            // Objets critiques
            if item.score > 0.7 {
                criticalObjects.append(frenchLabel)
            }

            // Objets de navigation
            if ["traffic light", "traffic sign", "pedestrian crossing"].contains(object.label) {
                navigationObjects.append(frenchLabel)
            }
        }

        print("üìä R√©sum√© par type:")
        for (type, count) in objectsByType {
            print("   - \(type): \(count)")
        }

        print("üìä R√©sum√© par zone:")
        for (zone, objects) in objectsByZone {
            if !objects.isEmpty {
                print("   - \(zone): \(objects)")
            }
        }

        return SceneAnalysis(
            totalObjects: currentImportantObjects.count,
            objectsByType: objectsByType,
            objectsByZone: objectsByZone,
            distances: distances,
            criticalObjects: criticalObjects,
            navigationObjects: navigationObjects
        )
    }

    private func handlePresenceQuestion(_ question: ParsedQuestion, analysis: SceneAnalysis) -> String {
        print("‚ùì Traitement question PRESENCE:")

        guard let targetObject = question.targetObject else {
            print("   - Aucun objet sp√©cifique ‚Üí pr√©sence g√©n√©rale")
            
            if analysis.totalObjects == 0 {
                return "Non, aucun objet d√©tect√© actuellement"
            }
            
            // Cr√©er une liste des objets d√©tect√©s avec leurs quantit√©s
            let objectsList = analysis.objectsByType.map { type, count in
                if count == 1 {
                    return "une \(type)"
                } else {
                    return "\(count) \(type)s"
                }
            }
            
            if objectsList.count == 1 {
                return "Oui, je d√©tecte \(objectsList[0])"
            } else if objectsList.count == 2 {
                return "Oui, je d√©tecte \(objectsList[0]) et \(objectsList[1])"
            } else {
                let allButLast = objectsList.dropLast().joined(separator: ", ")
                return "Oui, je d√©tecte \(allButLast) et \(objectsList.last!)"
            }
        }

        let frenchLabel = translateToFrench(targetObject)
        let count = analysis.objectsByType[frenchLabel] ?? 0

        print("   - Objet recherch√©: '\(targetObject)' ‚Üí '\(frenchLabel)'")
        print("   - Pr√©sence: \(count > 0 ? "OUI (\(count))" : "NON")")
        print("   - Objets disponibles: \(Array(analysis.objectsByType.keys))")

        if count > 0 {
            let location = findObjectLocation(targetObject, in: analysis)
            return count == 1 ?
                "Oui, je vois une \(frenchLabel)\(location.isEmpty ? "" : " \(location)")" :
                "Oui, je vois \(count) \(frenchLabel)s"
        } else {
            return "Non, je ne vois pas de \(frenchLabel) actuellement"
        }
    }

    private func handleCountQuestion(_ question: ParsedQuestion, analysis: SceneAnalysis) -> String {
        guard let targetObject = question.targetObject else {
            let total = analysis.totalObjects
            if total == 0 {
                return "Je ne d√©tecte aucun objet actuellement"
            } else if total == 1 {
                return "Je d√©tecte un objet"
            } else {
                let summaryParts = analysis.objectsByType.map { type, count in
                    "\(count) \(type)\(count > 1 ? "s" : "")"
                }
                let summary = summaryParts.joined(separator: ", ")
                return "Je d√©tecte \(total) objets au total : \(summary)"
            }
        }

        let frenchLabel = translateToFrench(targetObject)
        let count = analysis.objectsByType[frenchLabel] ?? 0

        switch count {
        case 0:
            return "Aucune \(frenchLabel) d√©tect√©e"
        case 1:
            return "Une \(frenchLabel)"
        default:
            return "\(count) \(frenchLabel)s"
        }
    }

    private func handleLocationQuestion(_ question: ParsedQuestion, analysis: SceneAnalysis) -> String {
        guard let targetObject = question.targetObject else {
            return generateGeneralLocationResponse(analysis: analysis)
        }

        let frenchLabel = translateToFrench(targetObject)
        let count = analysis.objectsByType[frenchLabel] ?? 0

        if count == 0 {
            return "Je ne vois pas de \(frenchLabel) actuellement"
        }

        let location = findObjectLocation(targetObject, in: analysis)
        let distance = findObjectDistance(targetObject, in: analysis)

        var response = "La \(frenchLabel) est \(location.isEmpty ? "visible" : location)"
        if !distance.isEmpty {
            response += " \(distance)"
        }

        if count > 1 {
            response = "Je vois \(count) \(frenchLabel)s. " + response.replacingOccurrences(of: "La \(frenchLabel)", with: "L'une d'elles")
        }

        return response
    }

    private func handleDescriptionQuestion(analysis: SceneAnalysis) -> String {
        if analysis.totalObjects == 0 {
            return "Je ne vois rien de particulier devant vous"
        }

        let frontObjects = analysis.objectsByZone["devant"] ?? []

        if frontObjects.isEmpty {
            return "Rien directement devant vous, mais je d√©tecte des objets sur les c√¥t√©s"
        }

        let objectGroups = Dictionary(grouping: frontObjects) { $0 }
        let objectList = objectGroups.map { obj, occurrences in
            let count = occurrences.count
            return count > 1 ? "\(count) \(obj)s" : "une \(obj)"
        }.joined(separator: ", ")

        return "Devant vous, je vois : \(objectList)"
    }

    private func handleSceneOverviewQuestion(analysis: SceneAnalysis) -> String {
        if analysis.totalObjects == 0 {
            return "La sc√®ne est calme, aucun objet d√©tect√© actuellement"
        }

        var description: [String] = []

        if analysis.criticalObjects.count > 0 {
            description.append("Attention, objets proches d√©tect√©s")
        }

        for (zone, objects) in analysis.objectsByZone {
            if !objects.isEmpty {
                let uniqueObjects = Dictionary(grouping: objects) { $0 }
                    .map { obj, list in list.count > 1 ? "\(list.count) \(obj)s" : "une \(obj)" }
                    .joined(separator: ", ")
                description.append("\(zone) : \(uniqueObjects)")
            }
        }

        if !analysis.navigationObjects.isEmpty {
            description.append("Signalisation pr√©sente")
        }

        let result = description.isEmpty ?
            "Environnement calme avec \(analysis.totalObjects) objet\(analysis.totalObjects > 1 ? "s" : "") d√©tect√©\(analysis.totalObjects > 1 ? "s" : "")" :
            description.joined(separator: ". ")

        return result
    }

    private func handleSpecificQuestion(_ question: ParsedQuestion, analysis: SceneAnalysis) -> String {
        return handleLocationQuestion(question, analysis: analysis)
    }

    private func handleUnknownQuestion(_ question: ParsedQuestion, analysis: SceneAnalysis) -> String {
        let text = question.originalText.lowercased()

        if text.contains("aide") || text.contains("help") {
            return "Vous pouvez me demander s'il y a des objets, combien il y en a, o√π ils sont, ou me demander de d√©crire la sc√®ne"
        }

        if analysis.totalObjects == 0 {
            return "Je ne d√©tecte aucun objet actuellement. Reformulez votre question si besoin"
        }

        let mainObjectsList = Array(analysis.objectsByType.prefix(3))
        let mainObjects = mainObjectsList.map { type, count in
            "\(count) \(type)\(count > 1 ? "s" : "")"
        }.joined(separator: ", ")

        return "Je ne suis pas s√ªr de comprendre. Actuellement je vois : \(mainObjects)"
    }

    // MARK: - M√©thodes utilitaires

    private func translateToFrench(_ englishLabel: String) -> String {
        for (english, frenchVariants) in objectTranslations {
            if english == englishLabel {
                return frenchVariants.first ?? englishLabel
            }
        }
        return englishLabel
    }

    private func getZoneFromBoundingBox(_ rect: CGRect) -> String {
        let centerX = rect.midX

        if centerX < 0.3 {
            return "gauche"
        } else if centerX > 0.7 {
            return "droite"
        } else {
            return "devant"
        }
    }

    private func findObjectLocation(_ objectType: String, in analysis: SceneAnalysis) -> String {
        let frenchLabel = translateToFrench(objectType)

        for (zone, objects) in analysis.objectsByZone {
            if objects.contains(frenchLabel) {
                return "√† \(zone == "devant" ? "votre avant" : zone)"
            }
        }
        return ""
    }

    private func findObjectDistance(_ objectType: String, in analysis: SceneAnalysis) -> String {
        let frenchLabel = translateToFrench(objectType)
        var closestDistance: Float?

        for (key, distance) in analysis.distances {
            if key.contains(frenchLabel) {
                if closestDistance == nil || distance < closestDistance! {
                    closestDistance = distance
                }
            }
        }

        guard let distance = closestDistance else { return "" }

        if distance < 1.0 {
            return "√† \(Int(distance * 100)) centim√®tres"
        } else if distance < 10.0 {
            return "√† \(String(format: "%.1f", distance)) m√®tres"
        } else {
            return "√† \(Int(distance)) m√®tres"
        }
    }

    private func generateGeneralLocationResponse(analysis: SceneAnalysis) -> String {
        if analysis.totalObjects == 0 {
            return "Aucun objet d√©tect√© pour localiser"
        }

        var locations: [String] = []

        for (zone, objects) in analysis.objectsByZone {
            if !objects.isEmpty {
                let count = objects.count
                let zoneDescription = zone == "devant" ? "devant vous" : "√† \(zone)"
                locations.append("\(count) objet\(count > 1 ? "s" : "") \(zoneDescription)")
            }
        }

        return locations.isEmpty ? "Objets d√©tect√©s sans position pr√©cise" : locations.joined(separator: ", ")
    }

    // MARK: - Interface publique pour les stats

    func getStats() -> String {
        let statusText: String

        if isRecovering {
            statusText = "üîÑ R√©cup√©ration en cours"
        } else if isListening {
            statusText = isWaitingForQuestion ? "üé§ √âcoute d'une question" : "üëÇ √âcoute active"
        } else if interactionEnabled {
            statusText = "‚úÖ Pr√™t (touchez le micro pour parler)"
        } else {
            statusText = "‚ùå Non disponible"
        }

        let errorInfo = currentRetryCount > 0 ? "\n‚ö†Ô∏è Erreurs r√©centes: \(currentRetryCount)/\(maxRetryAttempts)" : ""

        // Mode de reconnaissance (toujours local)
        var privacyInfo = "üîí Mode: LOCAL uniquement (100% priv√©)"
        if #available(iOS 13.0, *) {
            if let speechRecognizer = speechRecognizer {
                if !speechRecognizer.supportsOnDeviceRecognition {
                    privacyInfo = "‚ùå Mode local non support√© (interaction d√©sactiv√©e)"
                }
            }
        } else {
            privacyInfo = "‚ùå iOS 13+ requis pour mode local"
        }

        return """
        üé§ Interaction Vocale (confiance Apple):
           - √âtat: \(statusText)
           - Service: \(speechAvailable ? "‚úÖ Disponible" : "‚ùå Indisponible")
           - \(privacyInfo)
           - Derni√®re activit√©: "\(lastRecognizedText)"
           - Objets analys√©s: \(currentImportantObjects.count)\(errorInfo)

        üí° Mode d'emploi:
           1. Appui long sur l'√©cran (0.8s)
           2. Attendez le bip sonore
           3. Posez votre question clairement
           4. Apple g√®re automatiquement la finalisation

        ‚ùì Questions support√©es:
           - "Y a-t-il des voitures ?"
           - "Combien d'objets ?"
           - "O√π est le feu ?"
           - "D√©cris la sc√®ne"

        üîí Confidentialit√© garantie:
           - Aucune donn√©e audio envoy√©e sur internet
           - Traitement 100% local sur votre appareil
           - Pas d'√©coute continue (√©conomie batterie)

        üçé Syst√®me Apple:
           - D√©tection automatique des pauses
           - Finalisation intelligente des phrases
           - Timeout d'urgence: 30s (vs 8s avant)
        """
    }

    /// Indique si le service est pr√™t √† traiter une question
    func isReadyForQuestion() -> Bool {
        return interactionEnabled && speechAvailable && !isListening && !isRecovering
    }
}
