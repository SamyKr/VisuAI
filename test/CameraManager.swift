//
//  CameraManager.swift
//  test
//
//  Created by Samy üìç on 18/06/2025.
//

import AVFoundation
import Vision
import SwiftUI

// --- PROTOCOLE MIS √Ä JOUR POUR LIDAR ---
protocol CameraManagerDelegate {
    func didDetectObjects(_ detections: [(rect: CGRect, label: String, confidence: Float, distance: Float?)])
}
// -----------------------------

class CameraManager: NSObject, ObservableObject {
    @Published var isRunning = false
    @Published var hasPermission = false
    @Published var currentFPS: Double = 0.0  // Pour afficher les performances
    @Published var isLiDARAvailable = false
    @Published var isLiDAREnabled = false
    
    // Le 'weak' est toujours important ici pour √©viter les cycles de r√©tention
    // m√™me si le d√©l√©gu√© peut √™tre une struct (car la struct n'a pas de cycle de r√©tention directe avec le manager)
    var delegate: CameraManagerDelegate?
    
    private let captureSession = AVCaptureSession()
    private let videoDataOutput = AVCaptureVideoDataOutput()
    private let depthDataOutput = AVCaptureDepthDataOutput()  // Pour LiDAR
    private let sessionQueue = DispatchQueue(label: "camera.session.queue")
    private var previewLayer: AVCaptureVideoPreviewLayer?
    
    private let objectDetectionManager = ObjectDetectionManager()
    
    // Configuration des skip frames
    private var skipFrameCount = 5  // Par d√©faut, traite 1 frame sur 6
    private var frameCounter = 0
    
    // Donn√©es de profondeur pour LiDAR
    private var latestDepthData: CVPixelBuffer?
    private var currentDetectionsWithDistance: [(rect: CGRect, label: String, confidence: Float, distance: Float?)] = []
    
    override init() {
        super.init()
        checkLiDARAvailability()
        setupCaptureSession()
    }
    
    func requestPermission() {
        AVCaptureDevice.requestAccess(for: .video) { granted in
            DispatchQueue.main.async {
                self.hasPermission = granted
            }
        }
    }
    
    func startSession() {
        guard hasPermission else {
            requestPermission()
            return
        }
        
        sessionQueue.async {
            if !self.captureSession.isRunning {
                self.captureSession.startRunning()
                DispatchQueue.main.async {
                    self.isRunning = true
                }
            }
        }
    }
    
    func stopSession() {
        sessionQueue.async {
            if self.captureSession.isRunning {
                self.captureSession.stopRunning()
                DispatchQueue.main.async {
                    self.isRunning = false
                }
            }
        }
    }
    
    func getPreviewLayer() -> AVCaptureVideoPreviewLayer {
        if previewLayer == nil {
            previewLayer = AVCaptureVideoPreviewLayer(session: captureSession)
            previewLayer?.videoGravity = .resizeAspectFill
        }
        return previewLayer!
    }
    
    // Fonction pour obtenir les statistiques de performance
    func getPerformanceStats() -> String {
        return objectDetectionManager.getPerformanceStats()
    }
    
    // Fonction pour r√©initialiser les statistiques
    func resetPerformanceStats() {
        objectDetectionManager.resetStats()
    }
    
    // MARK: - Configuration Skip Frames
    func setSkipFrames(_ count: Int) {
        skipFrameCount = max(0, count)
        print("‚öôÔ∏è Skip frames d√©fini √†: \(skipFrameCount)")
    }
    
    func getSkipFrames() -> Int {
        return skipFrameCount
    }
    
    // MARK: - Configuration des classes actives
    func setActiveClasses(_ classes: [String]) {
        objectDetectionManager.setActiveClasses(classes)
    }
    
    func getActiveClasses() -> [String] {
        return objectDetectionManager.getActiveClasses()
    }
    
    // MARK: - LiDAR Configuration
    
    private func checkLiDARAvailability() {
        isLiDARAvailable = objectDetectionManager.isLiDARSupported()
        if isLiDARAvailable {
            isLiDAREnabled = true  // Activ√© par d√©faut si disponible
            objectDetectionManager.setLiDAREnabled(true)
            print("üì° LiDAR d√©tect√© et activ√©")
        } else {
            print("üì° LiDAR non disponible sur cet appareil")
        }
    }
    
    func setLiDAREnabled(_ enabled: Bool) {
        guard isLiDARAvailable else { return }
        isLiDAREnabled = enabled
        objectDetectionManager.setLiDAREnabled(enabled)
        
        // Reconfigurer la session cam√©ra si n√©cessaire
        if isRunning {
            sessionQueue.async {
                self.setupCaptureSession()
            }
        }
    }
    
    private func setupCaptureSession() {
        sessionQueue.async {
            self.captureSession.beginConfiguration()
            
            // Nettoyer les anciennes configurations
            for input in self.captureSession.inputs {
                self.captureSession.removeInput(input)
            }
            for output in self.captureSession.outputs {
                self.captureSession.removeOutput(output)
            }
            
            // Choisir la cam√©ra appropri√©e (avec LiDAR si disponible et activ√©)
            var videoDevice: AVCaptureDevice?
            
            if self.isLiDAREnabled && self.isLiDARAvailable {
                // Cam√©ra avec LiDAR
                videoDevice = AVCaptureDevice.default(.builtInLiDARDepthCamera, for: .video, position: .back)
                print("üì° Configuration cam√©ra avec LiDAR")
            }
            
            // Fallback vers la cam√©ra classique
            if videoDevice == nil {
                videoDevice = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back)
                print("üì∑ Configuration cam√©ra standard")
            }
            
            guard let device = videoDevice,
                  let videoDeviceInput = try? AVCaptureDeviceInput(device: device),
                  self.captureSession.canAddInput(videoDeviceInput) else {
                print("‚ùå Impossible de configurer l'entr√©e vid√©o")
                self.captureSession.commitConfiguration()
                return
            }
            
            self.captureSession.addInput(videoDeviceInput)
            
            // Configurer la sortie vid√©o
            if self.captureSession.canAddOutput(self.videoDataOutput) {
                self.captureSession.addOutput(self.videoDataOutput)
                
                self.videoDataOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "video.processing.queue"))
                self.videoDataOutput.videoSettings = [
                    kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA
                ]
            }
            
            // Configurer la sortie depth si LiDAR est activ√©
            if self.isLiDAREnabled && self.isLiDARAvailable {
                if self.captureSession.canAddOutput(self.depthDataOutput) {
                    self.captureSession.addOutput(self.depthDataOutput)
                    self.depthDataOutput.setDelegate(self, callbackQueue: DispatchQueue(label: "depth.processing.queue"))
                    
                    // Synchroniser les donn√©es vid√©o et depth
                    if let connection = self.depthDataOutput.connection(with: .depthData) {
                        connection.isEnabled = true
                    }
                    
                    print("üì° Sortie depth configur√©e")
                }
            }
            
            self.captureSession.commitConfiguration()
        }
    }
    
    // Stocker les d√©tections avec distance pour l'acc√®s depuis l'interface
    private func storeDetectionsWithDistance(_ detections: [(rect: CGRect, label: String, confidence: Float, distance: Float?)]) {
        currentDetectionsWithDistance = detections
    }
    
    // Obtenir les d√©tections avec distance
    func getDetectionsWithDistance() -> [(rect: CGRect, label: String, confidence: Float, distance: Float?)] {
        return currentDetectionsWithDistance
    }
}

// MARK: - AVCaptureVideoDataOutputSampleBufferDelegate
extension CameraManager: AVCaptureVideoDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        
        // Syst√®me de skip frames configurable
        frameCounter += 1
        guard frameCounter % (skipFrameCount + 1) == 0 else { return }
        
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        
        // Passer les donn√©es de profondeur si disponibles
        objectDetectionManager.detectObjects(in: pixelBuffer, depthData: latestDepthData) { [weak self] detections, inferenceTime in
            DispatchQueue.main.async {
                // Mettre √† jour le FPS pour l'affichage
                self?.currentFPS = 1000.0 / inferenceTime
                
                // Transmettre directement les d√©tections avec distance au delegate
                self?.delegate?.didDetectObjects(detections)
                
                // Stocker les distances pour l'affichage si n√©cessaire
                self?.storeDetectionsWithDistance(detections)
            }
        }
    }
}

// MARK: - AVCaptureDepthDataOutputDelegate
extension CameraManager: AVCaptureDepthDataOutputDelegate {
    func depthDataOutput(_ output: AVCaptureDepthDataOutput, didOutput depthData: AVDepthData, timestamp: CMTime, connection: AVCaptureConnection) {
        
        // Convertir les donn√©es de profondeur en CVPixelBuffer
        let depthPixelBuffer = depthData.depthDataMap
        
        // Stocker les derni√®res donn√©es de profondeur
        latestDepthData = depthPixelBuffer
        
        // Debug : V√©rifier que les donn√©es arrivent
        let depthWidth = CVPixelBufferGetWidth(depthPixelBuffer)
        let depthHeight = CVPixelBufferGetHeight(depthPixelBuffer)
        print("üì° CameraManager: Donn√©es LiDAR re√ßues: \(depthWidth)x\(depthHeight)")
    }
}
