import CoreML
import Vision
import UIKit
import AVFoundation

class ObjectDetectionManager {
    private var model: VNCoreMLModel?
    
    // Configuration de d√©tection am√©lior√©e
    private let confidenceThreshold: Float = 0.5  // Plus strict
    private let maxDetections = 10  // Limite le nombre de d√©tections
    
    // Classes √† ignorer par d√©faut (modifiable)
    private var ignoredClasses = Set(["building", "vegetation", "road", "sidewalk", "ground", "wall", "fence"])
    private var activeClasses: Set<String> = []  // Si vide, toutes les classes sont actives
    
    // Configuration LiDAR
    private var isLiDAREnabled = false
    private var currentDepthData: CVPixelBuffer?
    
    // Statistiques de performance
    private var inferenceHistory: [Double] = []
    private let maxHistorySize = 100
    
    // Taille du mod√®le
    private let modelInputSize = CGSize(width: 640, height: 640)
    
    init() {
        loadModel()
    }
    
    private func loadModel() {
        do {
            let config = MLModelConfiguration()
            
            config.setValue(1, forKey: "experimentalMLE5EngineUsage")
            
            guard let modelURL = Bundle.main.url(forResource: "last", withExtension: "mlmodelc") else {
                print("‚ùå Mod√®le 'last.mlmodelc' non trouv√© dans le bundle")
                return
            }
            
            print("‚úÖ Mod√®le compil√© trouv√©: last.mlmodelc")
            
            let mlModel = try MLModel(contentsOf: modelURL, configuration: config)
            self.model = try VNCoreMLModel(for: mlModel)
            
        } catch {
            print("‚ùå Erreur lors du chargement du mod√®le: \(error)")
        }
    }
    
    // MARK: - M√©thodes publiques de d√©tection
    
    // Version pour images (sans LiDAR)
    func detectObjects(in image: UIImage, completion: @escaping ([(rect: CGRect, label: String, confidence: Float)], Double) -> Void) {
        detectObjectsWithDistance(in: image) { detectionsWithDistance, time in
            // Convertir vers l'ancien format sans distance
            let detections = detectionsWithDistance.map { (rect: $0.rect, label: $0.label, confidence: $0.confidence) }
            completion(detections, time)
        }
    }
    
    // Version pour pixelBuffer avec LiDAR optionnel
    func detectObjects(in pixelBuffer: CVPixelBuffer, depthData: CVPixelBuffer? = nil, completion: @escaping ([(rect: CGRect, label: String, confidence: Float, distance: Float?)], Double) -> Void) {
        guard let model = model else {
            print("‚ùå Mod√®le non charg√©")
            completion([], 0.0)
            return
        }
        
        // Stocker les donn√©es de profondeur pour le calcul de distance
        currentDepthData = depthData
        if depthData != nil {
            print("üì° ObjectDetectionManager: Donn√©es depth re√ßues")
        }
        
        // Mesure du temps de pr√©processing
        let preprocessStart = CFAbsoluteTimeGetCurrent()
        let ciImage = preprocessPixelBuffer(pixelBuffer)
        let preprocessTime = (CFAbsoluteTimeGetCurrent() - preprocessStart) * 1000
        
        let originalSize = CGSize(width: CVPixelBufferGetWidth(pixelBuffer),
                                 height: CVPixelBufferGetHeight(pixelBuffer))
        
        // Pr√©processing optimis√© pour 640x640
        let preprocessedImage = preprocessImageFor640x640(ciImage)
        performDetectionWithDistance(on: preprocessedImage, with: model, originalImageSize: originalSize, preprocessTime: preprocessTime, completion: completion)
    }
    
    // Version pour vid√©o (utilis√©e par VideoDetectionManager)
    func detectObjectsInVideo(in image: UIImage, completion: @escaping ([(rect: CGRect, label: String, confidence: Float, distance: Float?)], Double) -> Void) {
        detectObjectsWithDistance(in: image, completion: completion)
    }
    
    // MARK: - M√©thodes priv√©es de d√©tection avec distance
    
    private func detectObjectsWithDistance(in image: UIImage, completion: @escaping ([(rect: CGRect, label: String, confidence: Float, distance: Float?)], Double) -> Void) {
        guard let model = model else {
            print("‚ùå Mod√®le non charg√©")
            completion([], 0.0)
            return
        }
        
        guard let ciImage = CIImage(image: image) else {
            print("‚ùå Impossible de convertir l'image")
            completion([], 0.0)
            return
        }
        
        // Pr√©processing pour adapter √† 640x640
        let preprocessedImage = preprocessImageFor640x640(ciImage)
        performDetectionWithDistance(on: preprocessedImage, with: model, originalImageSize: image.size, completion: completion)
    }
    
    private func performDetectionWithDistance(on ciImage: CIImage, with model: VNCoreMLModel, originalImageSize: CGSize, preprocessTime: Double = 0.0, completion: @escaping ([(rect: CGRect, label: String, confidence: Float, distance: Float?)], Double) -> Void) {
        
        // D√©marrage du chrono pour l'inf√©rence compl√®te
        let totalStartTime = CFAbsoluteTimeGetCurrent()
        
        let request = VNCoreMLRequest(model: model) { request, error in
            // Fin du chrono
            let totalInferenceTime = (CFAbsoluteTimeGetCurrent() - totalStartTime) * 1000 // en ms
            
            if let error = error {
                print("‚ùå Erreur de d√©tection: \(error)")
                completion([], totalInferenceTime)
                return
            }
            
            guard let results = request.results as? [VNRecognizedObjectObservation] else {
                print("‚ùå Aucun r√©sultat de d√©tection")
                completion([], totalInferenceTime)
                return
            }
            
            // Mesure du temps de post-processing
            let postProcessStart = CFAbsoluteTimeGetCurrent()
            let detections = self.processDetectionsWithDistance(results, originalImageSize: originalImageSize)
            let postProcessTime = (CFAbsoluteTimeGetCurrent() - postProcessStart) * 1000
            
            // Calcul du temps d'inf√©rence pure (sans post-processing)
            let pureInferenceTime = totalInferenceTime - postProcessTime - preprocessTime
            
            // Stockage des statistiques
            self.updateInferenceStats(totalInferenceTime)
            
            // Affichage d√©taill√© des temps
            print("üéØ YOLOv11 (640x640): \(detections.count) objets d√©tect√©s")
            print("‚è±Ô∏è Temps d'ex√©cution:")
            if preprocessTime > 0 {
                print("   - Pr√©processing: \(String(format: "%.1f", preprocessTime))ms")
            }
            print("   - Inf√©rence pure: \(String(format: "%.1f", pureInferenceTime))ms")
            print("   - Post-processing: \(String(format: "%.1f", postProcessTime))ms")
            print("   - TOTAL: \(String(format: "%.1f", totalInferenceTime))ms")
            print("   - FPS estim√©: \(String(format: "%.1f", 1000.0 / totalInferenceTime))")
            
            // Affichage des statistiques moyennes
            if let avgTime = self.getAverageInferenceTime() {
                print("   - Moyenne (derni√®res \(self.inferenceHistory.count)): \(String(format: "%.1f", avgTime))ms")
            }
            
            // DEBUG: Affichage d√©taill√© des d√©tections avec distance
            for detection in detections {
                let distanceText = detection.distance != nil ? " - \(String(format: "%.1f", detection.distance!))m" : " - NO DIST"
                print("   - \(detection.label): \(String(format: "%.1f", detection.confidence * 100))%\(distanceText)")
            }
            
            completion(detections, totalInferenceTime)
        }
        
        // Configuration de la requ√™te
        request.imageCropAndScaleOption = .scaleFit  // Maintient l'aspect ratio
        
        // Configuration additionnelle pour am√©liorer les performances
        if #available(iOS 14.0, *) {
            request.usesCPUOnly = false  // Utilise le GPU/Neural Engine
        }
        
        do {
            let handler = VNImageRequestHandler(ciImage: ciImage, options: [:])
            try handler.perform([request])
        } catch {
            let errorTime = (CFAbsoluteTimeGetCurrent() - totalStartTime) * 1000
            print("‚ùå √âchec de la d√©tection: \(error)")
            completion([], errorTime)
        }
    }
    
    // MARK: - Preprocessing
    
    private func preprocessPixelBuffer(_ pixelBuffer: CVPixelBuffer) -> CIImage {
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        
        // Correction d'orientation pour la cam√©ra
        let orientedImage = ciImage.oriented(.right)  // Ajuste selon ton setup
        
        // Optionnel : am√©lioration de l'image
        return orientedImage
            .applyingFilter("CIColorControls", parameters: [
                "inputSaturation": 1.1,  // L√©g√®re am√©lioration des couleurs
                "inputBrightness": 0.0,
                "inputContrast": 1.1
            ])
    }
    
    private func preprocessImageFor640x640(_ ciImage: CIImage) -> CIImage {
        let imageSize = ciImage.extent.size
        
        // Calcul du ratio pour maintenir l'aspect ratio
        let scaleX = modelInputSize.width / imageSize.width
        let scaleY = modelInputSize.height / imageSize.height
        let scale = min(scaleX, scaleY)  // Utilise le plus petit ratio pour √©viter la d√©formation
        
        // Redimensionnement en gardant l'aspect ratio
        let scaledImage = ciImage.transformed(by: CGAffineTransform(scaleX: scale, y: scale))
        
        // Centrage dans un carr√© 640x640
        let scaledSize = CGSize(width: imageSize.width * scale, height: imageSize.height * scale)
        let offsetX = (modelInputSize.width - scaledSize.width) / 2
        let offsetY = (modelInputSize.height - scaledSize.height) / 2
        
        let centeredImage = scaledImage.transformed(by: CGAffineTransform(translationX: offsetX, y: offsetY))
        
        // Cr√©ation d'un fond noir 640x640
        let blackBackground = CIImage(color: CIColor.black).cropped(to: CGRect(origin: .zero, size: modelInputSize))
        
        // Composition de l'image centr√©e sur le fond noir
        let finalImage = centeredImage.composited(over: blackBackground)
        
        return finalImage
    }
    
    // MARK: - Processing avec distance
    
    private func processDetectionsWithDistance(_ results: [VNRecognizedObjectObservation], originalImageSize: CGSize) -> [(rect: CGRect, label: String, confidence: Float, distance: Float?)] {
        
        // Calcul des ratios pour reconvertir les coordonn√©es
        let scaleX = modelInputSize.width / originalImageSize.width
        let scaleY = modelInputSize.height / originalImageSize.height
        let scale = min(scaleX, scaleY)
        
        let scaledSize = CGSize(width: originalImageSize.width * scale, height: originalImageSize.height * scale)
        let offsetX = (modelInputSize.width - scaledSize.width) / 2
        let offsetY = (modelInputSize.height - scaledSize.height) / 2
        
        // Filtrage par confiance
        let filteredResults = results.filter { $0.confidence >= confidenceThreshold }
        
        // Tri par confiance (meilleurs d'abord)
        let sortedResults = filteredResults.sorted { $0.confidence > $1.confidence }
        
        // Limitation du nombre de d√©tections
        let limitedResults = Array(sortedResults.prefix(maxDetections))
        
        // Conversion avec correction des coordonn√©es et calcul de distance
        return limitedResults.compactMap { observation in
            let topLabel = observation.labels.first?.identifier ?? "object"
            let confidence = observation.confidence
            
            // V√©rifier si la classe est autoris√©e
            if !isClassAllowed(topLabel) {
                print("üö´ Classe filtr√©e: \(topLabel)")
                return nil
            }
            
            // R√©cup√©ration de la bounding box (normalis√©e 0-1)
            var boundingBox = observation.boundingBox
            
            // Correction des coordonn√©es pour l'image originale
            // Vision utilise un syst√®me de coordonn√©es avec origine en bas-gauche
            boundingBox.origin.y = 1.0 - boundingBox.origin.y - boundingBox.height
            
            // Validation de la taille minimale
            guard boundingBox.width > 0.01 && boundingBox.height > 0.01 else {
                return nil
            }
            
            // Calcul de la distance si LiDAR est activ√© et disponible
            let distance = calculateDistance(for: boundingBox, imageSize: originalImageSize)
            
            return (rect: boundingBox, label: topLabel, confidence: confidence, distance: distance)
        }
    }
    
    // MARK: - M√©thodes de statistiques de performance
    
    private func updateInferenceStats(_ inferenceTime: Double) {
        inferenceHistory.append(inferenceTime)
        
        // Maintenir un historique limit√©
        if inferenceHistory.count > maxHistorySize {
            inferenceHistory.removeFirst()
        }
    }
    
    func getAverageInferenceTime() -> Double? {
        guard !inferenceHistory.isEmpty else { return nil }
        return inferenceHistory.reduce(0, +) / Double(inferenceHistory.count)
    }
    
    func getMinMaxInferenceTime() -> (min: Double, max: Double)? {
        guard !inferenceHistory.isEmpty else { return nil }
        return (inferenceHistory.min()!, inferenceHistory.max()!)
    }
    
    func getPerformanceStats() -> String {
        guard !inferenceHistory.isEmpty else {
            return "üìä Aucune statistique disponible"
        }
        
        let avg = getAverageInferenceTime()!
        let (min, max) = getMinMaxInferenceTime()!
        let avgFPS = 1000.0 / avg
        
        var stats = "üìä Statistiques de performance (\(inferenceHistory.count) inf√©rences):\n"
        stats += "   - Temps moyen: \(String(format: "%.1f", avg))ms\n"
        stats += "   - Temps min: \(String(format: "%.1f", min))ms\n"
        stats += "   - Temps max: \(String(format: "%.1f", max))ms\n"
        stats += "   - FPS moyen: \(String(format: "%.1f", avgFPS))\n"
        
        // Calcul de la variance pour la stabilit√©
        let variance = inferenceHistory.map { pow($0 - avg, 2) }.reduce(0, +) / Double(inferenceHistory.count)
        let stdDev = sqrt(variance)
        stats += "   - √âcart-type: \(String(format: "%.1f", stdDev))ms"
        
        return stats
    }
    
    func resetStats() {
        inferenceHistory.removeAll()
        print("üìä Statistiques de performance r√©initialis√©es")
    }
    
    // MARK: - Configuration des classes ignor√©es
    
    func addIgnoredClass(_ className: String) {
        ignoredClasses.insert(className.lowercased())
        print("üö´ Classe ajout√©e √† la liste d'ignor√©s: \(className)")
    }
    
    func removeIgnoredClass(_ className: String) {
        ignoredClasses.remove(className.lowercased())
        print("‚úÖ Classe retir√©e de la liste d'ignor√©s: \(className)")
    }
    
    func getIgnoredClasses() -> [String] {
        return Array(ignoredClasses).sorted()
    }
    
    // MARK: - Configuration des classes actives
    
    func setActiveClasses(_ classes: [String]) {
        activeClasses = Set(classes.map { $0.lowercased() })
        print("‚öôÔ∏è Classes actives d√©finies: \(classes.count) classes")
    }
    
    func getActiveClasses() -> [String] {
        return Array(activeClasses).sorted()
    }
    
    private func isClassAllowed(_ className: String) -> Bool {
        let lowercaseName = className.lowercased()
        
        // V√©rifier si la classe est ignor√©e
        if ignoredClasses.contains(lowercaseName) {
            return false
        }
        
        // Si activeClasses est vide, toutes les classes non-ignor√©es sont autoris√©es
        if activeClasses.isEmpty {
            return true
        }
        
        // Sinon, v√©rifier si la classe est dans la liste active
        return activeClasses.contains(lowercaseName)
    }
    
    // MARK: - LiDAR Distance Calculation
    
    func setLiDAREnabled(_ enabled: Bool) {
        isLiDAREnabled = enabled
        print("üì° LiDAR \(enabled ? "activ√©" : "d√©sactiv√©")")
    }
    
    func isLiDARSupported() -> Bool {
        // V√©rifier si l'appareil supporte le LiDAR
        if #available(iOS 14.0, *) {
            return AVCaptureDevice.default(.builtInLiDARDepthCamera, for: .video, position: .back) != nil
        }
        return false
    }
    
    private func calculateDistance(for boundingBox: CGRect, imageSize: CGSize) -> Float? {
        guard isLiDAREnabled, let depthData = currentDepthData else {
            print("üö´ LiDAR: Pas de donn√©es depth (enabled: \(isLiDAREnabled), data: \(currentDepthData != nil))")
            return nil
        }
        
        print("üì° LiDAR: Calcul distance pour box: \(boundingBox)")
        
        // Convertir la bounding box en coordonn√©es pixel
        let centerX = Int((boundingBox.midX * imageSize.width).rounded())
        let centerY = Int((boundingBox.midY * imageSize.height).rounded())
        
        print("üì° LiDAR: Centre de la box: (\(centerX), \(centerY))")
        
        // √âchantillonner plusieurs points dans la bounding box pour obtenir une distance moyenne
        var depths: [Float] = []
        let sampleSize = 5 // Grille 5x5 dans la bounding box
        
        let boxWidth = Int((boundingBox.width * imageSize.width).rounded())
        let boxHeight = Int((boundingBox.height * imageSize.height).rounded())
        
        for i in 0..<sampleSize {
            for j in 0..<sampleSize {
                let x = centerX - boxWidth/4 + (i * boxWidth) / (sampleSize * 2)
                let y = centerY - boxHeight/4 + (j * boxHeight) / (sampleSize * 2)
                
                if let depth = getDepthValue(at: CGPoint(x: x, y: y), from: depthData, imageSize: imageSize) {
                    depths.append(depth)
                    print("üì° LiDAR: Point (\(x), \(y)) = \(depth)m")
                }
            }
        }
        
        // Retourner la m√©diane pour √©viter les valeurs aberrantes
        guard !depths.isEmpty else {
            print("üö´ LiDAR: Aucune valeur de profondeur trouv√©e")
            return nil
        }
        
        depths.sort()
        let medianIndex = depths.count / 2
        let distance = depths[medianIndex]
        print("‚úÖ LiDAR: Distance calcul√©e: \(distance)m")
        return distance
    }
    
    private func getDepthValue(at point: CGPoint, from depthData: CVPixelBuffer, imageSize: CGSize) -> Float? {
        let depthWidth = CVPixelBufferGetWidth(depthData)
        let depthHeight = CVPixelBufferGetHeight(depthData)
        
        // Normaliser les coordonn√©es vers la r√©solution du depth buffer
        let normalizedX = point.x / imageSize.width
        let normalizedY = point.y / imageSize.height
        
        let depthX = Int((normalizedX * CGFloat(depthWidth)).rounded())
        let depthY = Int((normalizedY * CGFloat(depthHeight)).rounded())
        
        // V√©rifier les limites
        guard depthX >= 0, depthX < depthWidth, depthY >= 0, depthY < depthHeight else {
            return nil
        }
        
        CVPixelBufferLockBaseAddress(depthData, .readOnly)
        defer { CVPixelBufferUnlockBaseAddress(depthData, .readOnly) }
        
        let baseAddress = CVPixelBufferGetBaseAddress(depthData)
        let bytesPerRow = CVPixelBufferGetBytesPerRow(depthData)
        
        // Le format de depth est g√©n√©ralement kCVPixelFormatType_DepthFloat32
        let pixelFormat = CVPixelBufferGetPixelFormatType(depthData)
        
        if pixelFormat == kCVPixelFormatType_DepthFloat32 {
            let depthPointer = baseAddress!.assumingMemoryBound(to: Float32.self)
            let pixelIndex = depthY * (bytesPerRow / MemoryLayout<Float32>.size) + depthX
            let depth = depthPointer[pixelIndex]
            
            // Filtrer les valeurs invalides (NaN, infini, ou trop grandes)
            guard depth.isFinite, depth > 0, depth < 50 else { return nil }
            
            return depth
        }
        
        return nil
    }
}
