import CoreML
import Vision
import UIKit
import AVFoundation

class ObjectDetectionManager {
    private var model: VNCoreMLModel?
    
    // Configuration de dÃ©tection amÃ©liorÃ©e
    private let confidenceThreshold: Float = 0.5  // Plus strict
    private let maxDetections = 10  // Limite le nombre de dÃ©tections
    
    // Classes Ã  ignorer par dÃ©faut (modifiable)
    private var ignoredClasses = Set(["building", "vegetation", "road", "sidewalk", "ground", "wall", "fence"])
    private var activeClasses: Set<String> = []  // Si vide, toutes les classes sont actives
    
    // Configuration LiDAR
    private var isLiDAREnabled = false
    private var currentDepthData: CVPixelBuffer?
    
    // Statistiques de performance
    private var inferenceHistory: [Double] = []
    private let maxHistorySize = 100
    
    // Taille du modÃ¨le
    private let modelInputSize = CGSize(width: 640, height: 640)
    
    init() {
        loadModel()
    }
    
    private func loadModel() {
        do {
            let config = MLModelConfiguration()
            
            config.setValue(1, forKey: "experimentalMLE5EngineUsage")
            
            guard let modelURL = Bundle.main.url(forResource: "last", withExtension: "mlmodelc") else {
                print("âŒ ModÃ¨le 'last.mlmodelc' non trouvÃ© dans le bundle")
                return
            }
            
            print("âœ… ModÃ¨le compilÃ© trouvÃ©: last.mlmodelc")
            
            let mlModel = try MLModel(contentsOf: modelURL, configuration: config)
            self.model = try VNCoreMLModel(for: mlModel)
            
        } catch {
            print("âŒ Erreur lors du chargement du modÃ¨le: \(error)")
        }
    }
    
    // MARK: - MÃ©thodes publiques de dÃ©tection
    
    // Version pour images (sans LiDAR)
    func detectObjects(in image: UIImage, completion: @escaping ([(rect: CGRect, label: String, confidence: Float)], Double) -> Void) {
        detectObjectsWithDistance(in: image) { detectionsWithDistance, time in
            // Convertir vers l'ancien format sans distance
            let detections = detectionsWithDistance.map { (rect: $0.rect, label: $0.label, confidence: $0.confidence) }
            completion(detections, time)
        }
    }
    
    // Version pour pixelBuffer avec LiDAR optionnel
    func detectObjects(in pixelBuffer: CVPixelBuffer, depthData: CVPixelBuffer? = nil, completion: @escaping ([(rect: CGRect, label: String, confidence: Float, distance: Float?)], Double) -> Void) {
        guard let model = model else {
            print("âŒ ModÃ¨le non chargÃ©")
            completion([], 0.0)
            return
        }
        
        // Stocker les donnÃ©es de profondeur pour le calcul de distance
        currentDepthData = depthData
        if depthData != nil {
            print("ðŸ“¡ ObjectDetectionManager: DonnÃ©es depth reÃ§ues")
        }
        
        // Mesure du temps de prÃ©processing
        let preprocessStart = CFAbsoluteTimeGetCurrent()
        let ciImage = preprocessPixelBuffer(pixelBuffer)
        let preprocessTime = (CFAbsoluteTimeGetCurrent() - preprocessStart) * 1000
        
        let originalSize = CGSize(width: CVPixelBufferGetWidth(pixelBuffer),
                                 height: CVPixelBufferGetHeight(pixelBuffer))
        
        // PrÃ©processing optimisÃ© pour 640x640
        let preprocessedImage = preprocessImageFor640x640(ciImage)
        performDetectionWithDistance(on: preprocessedImage, with: model, originalImageSize: originalSize, preprocessTime: preprocessTime, completion: completion)
    }
    
    // Version pour vidÃ©o (utilisÃ©e par VideoDetectionManager)
    func detectObjectsInVideo(in image: UIImage, completion: @escaping ([(rect: CGRect, label: String, confidence: Float, distance: Float?)], Double) -> Void) {
        detectObjectsWithDistance(in: image, completion: completion)
    }
    
    // MARK: - MÃ©thodes privÃ©es de dÃ©tection avec distance
    
    private func detectObjectsWithDistance(in image: UIImage, completion: @escaping ([(rect: CGRect, label: String, confidence: Float, distance: Float?)], Double) -> Void) {
        guard let model = model else {
            print("âŒ ModÃ¨le non chargÃ©")
            completion([], 0.0)
            return
        }
        
        guard let ciImage = CIImage(image: image) else {
            print("âŒ Impossible de convertir l'image")
            completion([], 0.0)
            return
        }
        
        // PrÃ©processing pour adapter Ã  640x640
        let preprocessedImage = preprocessImageFor640x640(ciImage)
        performDetectionWithDistance(on: preprocessedImage, with: model, originalImageSize: image.size, completion: completion)
    }
    
    private func performDetectionWithDistance(on ciImage: CIImage, with model: VNCoreMLModel, originalImageSize: CGSize, preprocessTime: Double = 0.0, completion: @escaping ([(rect: CGRect, label: String, confidence: Float, distance: Float?)], Double) -> Void) {
        
        // DÃ©marrage du chrono pour l'infÃ©rence complÃ¨te
        let totalStartTime = CFAbsoluteTimeGetCurrent()
        
        let request = VNCoreMLRequest(model: model) { request, error in
            // Fin du chrono
            let totalInferenceTime = (CFAbsoluteTimeGetCurrent() - totalStartTime) * 1000 // en ms
            
            if let error = error {
                print("âŒ Erreur de dÃ©tection: \(error)")
                completion([], totalInferenceTime)
                return
            }
            
            guard let results = request.results as? [VNRecognizedObjectObservation] else {
                print("âŒ Aucun rÃ©sultat de dÃ©tection")
                completion([], totalInferenceTime)
                return
            }
            
            // Mesure du temps de post-processing
            let postProcessStart = CFAbsoluteTimeGetCurrent()
            let detections = self.processDetectionsWithDistance(results, originalImageSize: originalImageSize)
            let postProcessTime = (CFAbsoluteTimeGetCurrent() - postProcessStart) * 1000
            
            // Calcul du temps d'infÃ©rence pure (sans post-processing)
            let pureInferenceTime = totalInferenceTime - postProcessTime - preprocessTime
            
            // Stockage des statistiques
            self.updateInferenceStats(totalInferenceTime)
            
            // Affichage dÃ©taillÃ© des temps
            print("ðŸŽ¯ YOLOv11 (640x640): \(detections.count) objets dÃ©tectÃ©s")
            print("â±ï¸ Temps d'exÃ©cution:")
            if preprocessTime > 0 {
                print("   - PrÃ©processing: \(String(format: "%.1f", preprocessTime))ms")
            }
            print("   - InfÃ©rence pure: \(String(format: "%.1f", pureInferenceTime))ms")
            print("   - Post-processing: \(String(format: "%.1f", postProcessTime))ms")
            print("   - TOTAL: \(String(format: "%.1f", totalInferenceTime))ms")
            print("   - FPS estimÃ©: \(String(format: "%.1f", 1000.0 / totalInferenceTime))")
            
            // Affichage des statistiques moyennes
            if let avgTime = self.getAverageInferenceTime() {
                print("   - Moyenne (derniÃ¨res \(self.inferenceHistory.count)): \(String(format: "%.1f", avgTime))ms")
            }
            
            // DEBUG: Affichage dÃ©taillÃ© des dÃ©tections avec distance
            for detection in detections {
                let distanceText = detection.distance != nil ? " - \(String(format: "%.1f", detection.distance!))m" : " - NO DIST"
                print("   - \(detection.label): \(String(format: "%.1f", detection.confidence * 100))%\(distanceText)")
            }
            
            completion(detections, totalInferenceTime)
        }
        
        // Configuration de la requÃªte
        request.imageCropAndScaleOption = .scaleFit  // Maintient l'aspect ratio
        
        // Configuration additionnelle pour amÃ©liorer les performances
        if #available(iOS 14.0, *) {
            request.usesCPUOnly = false  // Utilise le GPU/Neural Engine
        }
        
        do {
            let handler = VNImageRequestHandler(ciImage: ciImage, options: [:])
            try handler.perform([request])
        } catch {
            let errorTime = (CFAbsoluteTimeGetCurrent() - totalStartTime) * 1000
            print("âŒ Ã‰chec de la dÃ©tection: \(error)")
            completion([], errorTime)
        }
    }
    
    // MARK: - Preprocessing
    
    private func preprocessPixelBuffer(_ pixelBuffer: CVPixelBuffer) -> CIImage {
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        
        // Correction d'orientation pour la camÃ©ra
        let orientedImage = ciImage.oriented(.right)  // Ajuste selon ton setup
        
        // Optionnel : amÃ©lioration de l'image
        return orientedImage
            .applyingFilter("CIColorControls", parameters: [
                "inputSaturation": 1.1,  // LÃ©gÃ¨re amÃ©lioration des couleurs
                "inputBrightness": 0.0,
                "inputContrast": 1.1
            ])
    }
    
    private func preprocessImageFor640x640(_ ciImage: CIImage) -> CIImage {
        let imageSize = ciImage.extent.size
        
        // Calcul du ratio pour maintenir l'aspect ratio
        let scaleX = modelInputSize.width / imageSize.width
        let scaleY = modelInputSize.height / imageSize.height
        let scale = min(scaleX, scaleY)  // Utilise le plus petit ratio pour Ã©viter la dÃ©formation
        
        // Redimensionnement en gardant l'aspect ratio
        let scaledImage = ciImage.transformed(by: CGAffineTransform(scaleX: scale, y: scale))
        
        // Centrage dans un carrÃ© 640x640
        let scaledSize = CGSize(width: imageSize.width * scale, height: imageSize.height * scale)
        let offsetX = (modelInputSize.width - scaledSize.width) / 2
        let offsetY = (modelInputSize.height - scaledSize.height) / 2
        
        let centeredImage = scaledImage.transformed(by: CGAffineTransform(translationX: offsetX, y: offsetY))
        
        // CrÃ©ation d'un fond noir 640x640
        let blackBackground = CIImage(color: CIColor.black).cropped(to: CGRect(origin: .zero, size: modelInputSize))
        
        // Composition de l'image centrÃ©e sur le fond noir
        let finalImage = centeredImage.composited(over: blackBackground)
        
        return finalImage
    }
    
    // MARK: - Processing avec distance
    
    private func processDetectionsWithDistance(_ results: [VNRecognizedObjectObservation], originalImageSize: CGSize) -> [(rect: CGRect, label: String, confidence: Float, distance: Float?)] {
        
        // Calcul des ratios pour reconvertir les coordonnÃ©es
        let scaleX = modelInputSize.width / originalImageSize.width
        let scaleY = modelInputSize.height / originalImageSize.height
        let scale = min(scaleX, scaleY)
        
        let scaledSize = CGSize(width: originalImageSize.width * scale, height: originalImageSize.height * scale)
        let offsetX = (modelInputSize.width - scaledSize.width) / 2
        let offsetY = (modelInputSize.height - scaledSize.height) / 2
        
        // Filtrage par confiance
        let filteredResults = results.filter { $0.confidence >= confidenceThreshold }
        
        // Tri par confiance (meilleurs d'abord)
        let sortedResults = filteredResults.sorted { $0.confidence > $1.confidence }
        
        // Limitation du nombre de dÃ©tections
        let limitedResults = Array(sortedResults.prefix(maxDetections))
        
        // Conversion avec correction des coordonnÃ©es et calcul de distance
        return limitedResults.compactMap { observation in
            let topLabel = observation.labels.first?.identifier ?? "object"
            let confidence = observation.confidence
            
            // VÃ©rifier si la classe est autorisÃ©e
            if !isClassAllowed(topLabel) {
                print("ðŸš« Classe filtrÃ©e: \(topLabel)")
                return nil
            }
            
            // RÃ©cupÃ©ration de la bounding box (normalisÃ©e 0-1)
            var boundingBox = observation.boundingBox
            
            // Correction des coordonnÃ©es pour l'image originale
            // Vision utilise un systÃ¨me de coordonnÃ©es avec origine en bas-gauche
            boundingBox.origin.y = 1.0 - boundingBox.origin.y - boundingBox.height
            
            // Validation de la taille minimale
            guard boundingBox.width > 0.01 && boundingBox.height > 0.01 else {
                return nil
            }
            
            // Calcul de la distance si LiDAR est activÃ© et disponible
            let distance = calculateDistance(for: boundingBox, imageSize: originalImageSize)
            
            return (rect: boundingBox, label: topLabel, confidence: confidence, distance: distance)
        }
    }
    
    // MARK: - MÃ©thodes de statistiques de performance
    
    private func updateInferenceStats(_ inferenceTime: Double) {
        inferenceHistory.append(inferenceTime)
        
        // Maintenir un historique limitÃ©
        if inferenceHistory.count > maxHistorySize {
            inferenceHistory.removeFirst()
        }
    }
    
    func getAverageInferenceTime() -> Double? {
        guard !inferenceHistory.isEmpty else { return nil }
        return inferenceHistory.reduce(0, +) / Double(inferenceHistory.count)
    }
    
    func getMinMaxInferenceTime() -> (min: Double, max: Double)? {
        guard !inferenceHistory.isEmpty else { return nil }
        return (inferenceHistory.min()!, inferenceHistory.max()!)
    }
    
    func getPerformanceStats() -> String {
        guard !inferenceHistory.isEmpty else {
            return "ðŸ“Š Aucune statistique disponible"
        }
        
        let avg = getAverageInferenceTime()!
        let (min, max) = getMinMaxInferenceTime()!
        let avgFPS = 1000.0 / avg
        
        var stats = "ðŸ“Š Statistiques de performance (\(inferenceHistory.count) infÃ©rences):\n"
        stats += "   - Temps moyen: \(String(format: "%.1f", avg))ms\n"
        stats += "   - Temps min: \(String(format: "%.1f", min))ms\n"
        stats += "   - Temps max: \(String(format: "%.1f", max))ms\n"
        stats += "   - FPS moyen: \(String(format: "%.1f", avgFPS))\n"
        
        // Calcul de la variance pour la stabilitÃ©
        let variance = inferenceHistory.map { pow($0 - avg, 2) }.reduce(0, +) / Double(inferenceHistory.count)
        let stdDev = sqrt(variance)
        stats += "   - Ã‰cart-type: \(String(format: "%.1f", stdDev))ms"
        
        return stats
    }
    
    func resetStats() {
        inferenceHistory.removeAll()
        print("ðŸ“Š Statistiques de performance rÃ©initialisÃ©es")
    }
    
    // MARK: - Configuration des classes ignorÃ©es
    
    func addIgnoredClass(_ className: String) {
        ignoredClasses.insert(className.lowercased())
        print("ðŸš« Classe ajoutÃ©e Ã  la liste d'ignorÃ©s: \(className)")
    }
    
    func removeIgnoredClass(_ className: String) {
        ignoredClasses.remove(className.lowercased())
        print("âœ… Classe retirÃ©e de la liste d'ignorÃ©s: \(className)")
    }
    
    func getIgnoredClasses() -> [String] {
        return Array(ignoredClasses).sorted()
    }
    
    // MARK: - Configuration des classes actives
    
    func setActiveClasses(_ classes: [String]) {
        activeClasses = Set(classes.map { $0.lowercased() })
        print("âš™ï¸ Classes actives dÃ©finies: \(classes.count) classes")
    }
    
    func getActiveClasses() -> [String] {
        return Array(activeClasses).sorted()
    }
    
    private func isClassAllowed(_ className: String) -> Bool {
        let lowercaseName = className.lowercased()
        
        // VÃ©rifier si la classe est ignorÃ©e
        if ignoredClasses.contains(lowercaseName) {
            return false
        }
        
        // Si activeClasses est vide, toutes les classes non-ignorÃ©es sont autorisÃ©es
        if activeClasses.isEmpty {
            return true
        }
        
        // Sinon, vÃ©rifier si la classe est dans la liste active
        return activeClasses.contains(lowercaseName)
    }
    
    // MARK: - LiDAR Distance Calculation
    
    func setLiDAREnabled(_ enabled: Bool) {
        isLiDAREnabled = enabled
        print("ðŸ“¡ LiDAR \(enabled ? "activÃ©" : "dÃ©sactivÃ©")")
    }
    
    func isLiDARSupported() -> Bool {
        // VÃ©rifier si l'appareil supporte le LiDAR
        if #available(iOS 14.0, *) {
            return AVCaptureDevice.default(.builtInLiDARDepthCamera, for: .video, position: .back) != nil
        }
        return false
    }
    
    private func calculateDistance(for boundingBox: CGRect, imageSize: CGSize) -> Float? {
        guard isLiDAREnabled, let depthData = currentDepthData else {
            print("ðŸš« LiDAR: Pas de donnÃ©es depth (enabled: \(isLiDAREnabled), data: \(currentDepthData != nil))")
            return nil
        }
        
        print("ðŸ“¡ LiDAR: Calcul distance pour box: \(boundingBox)")
        
        // Convertir la bounding box en coordonnÃ©es pixel
        let centerX = Int((boundingBox.midX * imageSize.width).rounded())
        let centerY = Int((boundingBox.midY * imageSize.height).rounded())
        
        print("ðŸ“¡ LiDAR: Centre de la box: (\(centerX), \(centerY))")
        
        // Ã‰chantillonner plusieurs points dans la bounding box pour obtenir une distance moyenne
        var depths: [Float] = []
        let sampleSize = 5 // Grille 5x5 dans la bounding box
        
        let boxWidth = Int((boundingBox.width * imageSize.width).rounded())
        let boxHeight = Int((boundingBox.height * imageSize.height).rounded())
        
        for i in 0..<sampleSize {
            for j in 0..<sampleSize {
                let x = centerX - boxWidth/4 + (i * boxWidth) / (sampleSize * 2)
                let y = centerY - boxHeight/4 + (j * boxHeight) / (sampleSize * 2)
                
                if let depth = getDepthValue(at: CGPoint(x: x, y: y), from: depthData, imageSize: imageSize) {
                    depths.append(depth)
                    print("ðŸ“¡ LiDAR: Point (\(x), \(y)) = \(depth)m")
                }
            }
        }
        
        // Retourner la mÃ©diane pour Ã©viter les valeurs aberrantes
        guard !depths.isEmpty else {
            print("ðŸš« LiDAR: Aucune valeur de profondeur trouvÃ©e")
            return nil
        }
        
        depths.sort()
        let medianIndex = depths.count / 2
        let distance = depths[medianIndex]
        print("âœ… LiDAR: Distance calculÃ©e: \(distance)m")
        return distance
    }
    
    private func getDepthValue(at point: CGPoint, from depthData: CVPixelBuffer, imageSize: CGSize) -> Float? {
        let depthWidth = CVPixelBufferGetWidth(depthData)
        let depthHeight = CVPixelBufferGetHeight(depthData)
        
        // Normaliser les coordonnÃ©es vers la rÃ©solution du depth buffer
        let normalizedX = point.x / imageSize.width
        let normalizedY = point.y / imageSize.height
        
        let depthX = Int((normalizedX * CGFloat(depthWidth)).rounded())
        let depthY = Int((normalizedY * CGFloat(depthHeight)).rounded())
        
        // VÃ©rifier les limites
        guard depthX >= 0, depthX < depthWidth, depthY >= 0, depthY < depthHeight else {
            return nil
        }
        
        CVPixelBufferLockBaseAddress(depthData, .readOnly)
        defer { CVPixelBufferUnlockBaseAddress(depthData, .readOnly) }
        
        let baseAddress = CVPixelBufferGetBaseAddress(depthData)
        let bytesPerRow = CVPixelBufferGetBytesPerRow(depthData)
        
        // Le format de depth est gÃ©nÃ©ralement kCVPixelFormatType_DepthFloat32
        let pixelFormat = CVPixelBufferGetPixelFormatType(depthData)
        
        if pixelFormat == kCVPixelFormatType_DepthFloat32 {
            let depthPointer = baseAddress!.assumingMemoryBound(to: Float32.self)
            let pixelIndex = depthY * (bytesPerRow / MemoryLayout<Float32>.size) + depthX
            let depth = depthPointer[pixelIndex]
            
            // Filtrer les valeurs invalides (NaN, infini, ou trop grandes)
            guard depth.isFinite, depth > 0, depth < 50 else { return nil }
            
            return depth
        }
        
        return nil
    }
}
